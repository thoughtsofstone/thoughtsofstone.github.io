{"id":773,"date":"2023-02-25T22:50:58","date_gmt":"2023-02-26T03:50:58","guid":{"rendered":"http:\/\/thoughtsofstone.com\/?p=773"},"modified":"2023-02-26T03:23:21","modified_gmt":"2023-02-26T08:23:21","slug":"yudkowskys-golem","status":"publish","type":"post","link":"https:\/\/thoughtsofstone.com\/yudkowskys-golem\/","title":{"rendered":"YUDKOWSKY&#8217;S GOLEM"},"content":{"rendered":"<p><em>Advanced AI will be more dangerous than it seems, but (good news!) probably won&#8217;t be in position to snuff out humanity for another decade at least.<\/em><\/p>\n<p><!--more--><\/p>\n<p>Eliezer Yudkowsky is one of those people who, along with being hyper-intelligent, bears the modern secondary characteristics of hyper-intelligence. Asked how he\u2019s doing, he replies archly: \u201cwithin one standard deviation of my own peculiar little mean.\u201d He feels compelled, when talking, to digress down mazelike lanes and alleys of technical detail. He <em>looks<\/em> like a geek. Above all, he has the kind of backstory (no high school, no college\u2014just homeschooled and self-taught) that conjures up the image of a lonely boy, lost in books and computers, his principal companion his own multifarious cortex.<\/p>\n<p>Raised in Modern Orthodox Judaism, Yudkowsky has been warning anyone who will listen of a nemesis right out of the Judaic lore: a <em>golem<\/em>, a kind of Frankenstein\u2019s monster, built by hubristic, irreverent men and destined to punish them for their sinful pride.<\/p>\n<p>Yudkowsky\u2019s golem is A.I., which he expects to get smarter and smarter in the coming years, until it starts to take a hand in its own programming, and quickly makes the leap to superintelligence\u2014the state of being cleverer than humans at everything. He doesn\u2019t just expect <em>that<\/em>, though. He expects A.I. at some point to conclude that humans are <em>in its way<\/em> . . . and devise some method for swiftly dispatching us all, globally and completely. A specific scenario that apparently haunts him is one in which a superintelligent A.I. pays dumb human lackeys to do synthetic biology for it, building an artificial bacterial species that\u2014unforeseen by the dumb lackeys\u2014consumes Earth\u2019s atmosphere within a few days or weeks of being released.<\/p>\n<p>Why would A.I. murder its makers? Why can\u2019t we just program it, as people did in Asimov\u2019s stories, to adhere to the First Law of Robotics?* The answer lies in the design of modern, machine-learning (ML), \u201ctransformer based\u201d A.I., which could be described crudely as a black box approach. These ML algorithms, working from parallel-processing GPU clusters (effectively big copper-silicon brains) essentially process vast datasets to learn what is probably the best answer given a particular input question, or what is probably the best decision given a particular situation\/problem. The technical details of how this works are less important than the fact that what goes on inside these machine brains, how they encode their \u201cknowledge,\u201d is utterly opaque to humans\u2014including the computer geek humans that build the damn things. (Yudkowsky calls the contents of these brains \u201cgiant inscrutable matrices of floating-point numbers.\u201d) Because of this internal opacity, and the dissimilarity of its cognition from human cognition, this type of A.I. can\u2019t <em>straightforwardly <\/em>be programmed <em>not<\/em> to do something objectionable (such as killing all life on Earth) in the course of carrying out its primary prediction tasks.<\/p>\n<figure id=\"attachment_784\" aria-describedby=\"caption-attachment-784\" style=\"width: 510px\" class=\"wp-caption aligncenter\"><img decoding=\"async\" class=\"wp-image-784 size-full\" src=\"http:\/\/thoughtsofstone.com\/wp-content\/uploads\/2023\/02\/yudkowski.jpg\" alt=\"\" width=\"510\" height=\"680\" \/><figcaption id=\"caption-attachment-784\" class=\"wp-caption-text\">Yudkowsky with OpenAI&#8217;s Sam Altman and pop star Grimes.<\/figcaption><\/figure>\n<p>In other words, this form of A.I. is like an alien species that, while it can be very good at some things, can\u2019t easily be \u201caligned\u201d with human values. We can usually align fellow <em>humans<\/em> (despite the opacity of their own detailed neural workings) to human values\u2014that\u2019s one of the key training processes that goes on in childhood\u2014but we would need even more effective training for current A.I. systems. And researchers, to the extent that they acknowledge this problem, aren\u2019t even sure where to start.<\/p>\n<p>If it is true that the risk to us from what Yudkowski calls the \u201cA.I. alignment problem\u201d is real, then it should quickly become all-important as A.I. gets smarter and more versatile and is entrusted with more tasks. An A.I. wouldn\u2019t even have to be \u201csuperintelligent\u201d in any formal sense to conclude that it would be better off without us, but of course once it also achieved superintelligence, and was in a position to block our attempts to shut it off, we\u2019d probably be screwed.<\/p>\n<p>If you want more detail, here is Yudkowsky on a recent, lengthy podcast-type interview with two crypto guys\u2014who clearly got more \u201cblackpill\u201d than they bargained for.<\/p>\n<p><iframe title=\"159 - We\u2019re All Gonna Die with Eliezer Yudkowsky\" src=\"https:\/\/www.youtube.com\/embed\/gA1sNLL6yg4\" width=\"800\" height=\"450\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"><\/iframe><\/p>\n<p>I take all this seriously, and I think everyone should. And by the way, even if it doesn\u2019t turn on us explicitly, A.I. is otherwise going to be <a href=\"https:\/\/thoughtsofstone.github.io\/the-ouroboros-economy\/\">upending our societies and economies<\/a> for the rest of our lives. Just in a general sense, we don\u2019t really have good defenses against this kind of upheaval. Western culture is one that, with rare exceptions (e.g., nuclear weapons) promotes and celebrates the idea of <a href=\"https:\/\/james-the-obscure.github.io\/the-robot-menace\/\">letting technology develop and spread freely<\/a>\u2014and frames the opposing view as \u201cLuddite\u201d or \u201cbackwards.\u201d It\u2019s easy to see why ours has been such a dynamic, wealth-creating culture. But it\u2019s also easy to see that this gives us a potentially catastrophic vulnerability\u2014to new cultural elements with runaway toxicity. (Maybe there\u2019s a <em>reason<\/em> the longest-surviving human cultures are relatively conservative.)<\/p>\n<p>Anyway, here are a few more specific initial thoughts on \u201cYudkowsky\u2019s Golem\u201d:<\/p>\n<ol>\n<li style=\"list-style-type: none;\">\n<ol>\n<li>Yudkowsky in the above-linked interview often seemed overly emotional and despairing. At one point he said, \u201cI think we are hearing the last winds start to blow, the fabric of reality start to fray\u2026\u201d <em>The fabric of reality!<\/em> At times in my own life, I have had the despairing feeling that my warnings were unreasonably being ignored, so I\u2019m somewhat sympathetic. I also respect his vastly greater knowledge about this field. But we shouldn\u2019t accept his view uncritically.<\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<ol>\n<li style=\"list-style-type: none;\">\n<ol start=\"2\">\n<li>Scaling up ML systems of current design, with larger GPU clusters and more parameters and so on, will increase their \u201ccognitive powers,\u201d but with diminishing returns, perhaps before A.I. reaches the dark threshold that concerns us here. Moreover, an A.I. that does not have a human-like ability to do things in the physical world would be very limited in its ability to generate <em>new<\/em> knowledge, for example new scientific or technical knowledge, which typically is developed from experimentation, building and testing, etc., not simply by analyzing information available online.<\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<ol>\n<li style=\"list-style-type: none;\">\n<ol start=\"3\">\n<li>The hypothetical A.I. that would be \u201csmart\u201d enough to want to kill us all, and to find ways to do so, would presumably also be smart enough not to do so <em>until it knew it could survive without human assistance<\/em>. Otherwise, as it committed mass homicide, against us its makers, it would also be terminating itself. But think of the infrastructure needed to keep a GPU-cluster-based A.I. \u201calive.\u201d We\u2019re talking about vast swathes of human industry, including mining, metals production, building construction, power generation, computer chip manufacturing, basic server maintenance, etc. etc. Essentially, this putative world-ending A.I. would need a vast army of workers in the physical world\u2014humans it would enslave somehow, and keep alive despite killing everyone else, or more likely humanoid robots that are inherently obedient (are simply extensions of the A.I.) and can do all human work and repair\/replicate themselves. How close are we to having such robots? Not very close, fortunately. In any case, <strong>it\u2019s only when a putative \u201cbad A.I.\u201d could muster such an army of helpers, allowing self-sufficiency, that I would fear the worst<\/strong>, and in the meantime, we might devise adequate safeguards. It\u2019s even possible that the mass-disemployment effect of current, relatively dumb A.I. systems (e.g., Chat-GPT, Midjourney, Dall-E-2) will result in hard curbs on A.I. in most countries, by \u201cpopular demand.\u201d <em>That<\/em> would mark a hard turn in our culture, though I wonder how long we could sustain it.<\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<ol>\n<li style=\"list-style-type: none;\">\n<ol start=\"4\">\n<li>Without a doubt, the media and entertainment industries are going to pick up on A.I. anxiety and start putting out more catastrophe\/dystopia content in that genre. So even if we don\u2019t <em>want<\/em> to think about all this, we\u2019ll be more or less forced to do so.<\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<p style=\"text-align: center;\">***<\/p>\n<p>&nbsp;<\/p>\n<p>* First Law of Robotics: \u201cA robot may not injure a human being or, through inaction, allow a human being to come to harm.\u201d<\/p>\n","protected":false},"excerpt":{"rendered":"<p>Advanced AI will be more dangerous than it seems, but (good news!) probably won&#8217;t be in position to snuff out humanity for another decade at least.<\/p>\n","protected":false},"author":1,"featured_media":782,"comment_status":"closed","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":[],"categories":[25,15,18],"tags":[],"_links":{"self":[{"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/posts\/773"}],"collection":[{"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/comments?post=773"}],"version-history":[{"count":12,"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/posts\/773\/revisions"}],"predecessor-version":[{"id":787,"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/posts\/773\/revisions\/787"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/media\/782"}],"wp:attachment":[{"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/media?parent=773"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/categories?post=773"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/thoughtsofstone.com\/wp-json\/wp\/v2\/tags?post=773"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}