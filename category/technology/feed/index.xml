<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>technology &#8211; Thoughts of Stone</title>
	<atom:link href="/category/technology/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>short essays, usually about humans</description>
	<lastBuildDate>Sat, 09 Sep 2023 21:43:42 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.2.2</generator>

<image>
	<url>/wp-content/uploads/2020/07/cropped-icon-32x32.jpg</url>
	<title>technology &#8211; Thoughts of Stone</title>
	<link>/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>THE IDEA THAT GOT AWAY</title>
		<link>/the-idea-that-got-away/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Wed, 30 Aug 2023 20:37:25 +0000</pubDate>
				<category><![CDATA[A.I.]]></category>
		<category><![CDATA[ideas]]></category>
		<category><![CDATA[psychology]]></category>
		<category><![CDATA[technology]]></category>
		<category><![CDATA[The Right]]></category>
		<category><![CDATA[wokeism]]></category>
		<category><![CDATA[women]]></category>
		<guid isPermaLink="false">/?p=842</guid>

					<description><![CDATA[A cautionary tale, and a plea for change &#160; &#160; “But are you strong enough now for a truly big fish?” —The Old Man and the Sea &#160; Have you ever had a Big Idea—an idea with the potential to transform the way people think about their society and culture? Imagine that you had such &#8230; <a href="/the-idea-that-got-away/" class="more-link">Continue reading<span class="screen-reader-text"> "THE IDEA THAT GOT AWAY"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>A cautionary tale, and a plea for change</em></p>
<p><span id="more-842"></span></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>“But are you strong enough now for a truly big fish?”</p>
<p>—<em>The Old Man and the Sea</em></p>
<p>&nbsp;</p>
<p>Have you ever had a Big Idea—an idea with the potential to transform the way people think about their society and culture?</p>
<p>Imagine that you had such a Big Idea, but you weren’t a professional opinionator and didn’t have an easy way of getting your Big Idea “out there” in front of a lot of readers.</p>
<p>Imagine too that your Big Idea was going to be controversial enough, in mainstream circles, that publication under your own name would almost certainly cost you your livelihood.</p>
<p>What would you do?</p>
<p>Here is what I did—and, as they say, don’t try this at home.</p>
<p>&nbsp;</p>
<p><strong>THE BIG IDEA</strong></p>
<p>&nbsp;</p>
<p>Before getting into the timeline of events, I want to emphasize that I have written this in part for the benefit of other, younger writers, who may read it someday and find it useful&#8212;as an account of a process that is relevant to their ambitions but seldom set forth in detail. More than that, though, I see this as a &#8220;case history&#8221; supporting an argument for changes in how we deal with new ideas (of the non-copyrightable, non-patentable variety) and incentivize their originators.</p>
<p>Now to the what and when: It all started early in the new millennium, after I returned to the US following a decades-long sojourn abroad. As I settled in, certain differences in American culture, compared to what I’d known as a young adult, started becoming apparent. Themes of “trauma” and suffering seemed much more prominent in the culture, from media to medicine. Public policy debates were often competitive exercises in projecting compassion, or “empathy,” in regard to supposed victims. Political correctness, a hypersensitive projection of concern for the disadvantaged, seemed out of control. Even in my own somewhat technical line of work, I noticed similar changes in tone and emphasis.</p>
<p>Eventually the proverbial lightbulb winked on. As I put it in an essay (“<a href="https://james-the-obscure.github.io/the-demise-of-guythink/">The Demise of Guythink</a>”) in late 2011:</p>
<p style="padding-left: 40px;">… these empathy-related changes in public discourse are due in large part to the recent, unprecedented entry of women into public life in Western countries. Women have not only the right to vote but also a presence in key areas of society—science, law, business, politics—as never before, and it would be hard to believe that their influence has not changed the culture, bending it towards their own cognitive style. People now use the jokey phrase “endangered white male” . . . but what may be truly endangered here is the male cognitive style.</p>
<p style="padding-left: 40px;">That may not be a good thing, if the male cognitive style evolved to be optimal for managing societies, while the female cognitive style is tuned for the rearing of children. There is a tendency in our culture now to treat empathy as a trait to be simply maximized. But “understanding and building systems,” as [Simon] Baron-Cohen puts it, is useful, too—and perhaps most if not all of our culture’s greatest failings now come not from a lack of empathy but from a failure to see how complex systems fit together, and how they may fly apart.</p>
<p style="padding-left: 40px;">What also worries me is that too much empathy, or other related aspects of the female cognitive style, may be—we don’t know; probably no scientist would go near this question—less compatible with the reasoned debate and calm analytical thinking that are presumably needed in a healthy democracy, or in any mature society. Several years ago, then-Harvard President Lawrence Summers (who was later a White House adviser) referred rather delicately to the possibility that male/female cognitive differences partly explain the relative lack of female professors in math and science; he was, in effect, shouted down and forced from his post….</p>
<p style="padding-left: 40px;">An inflexible, authoritarian, shout-them-down tendency is often said to be a feature of PC-think generally. PC-driven marches and protests (on campuses for example) typically are meant not to broaden a discourse but, rather, to repel or suppress an unwanted speaker—much as a mother, without any pretense of democracy or debate, would try to protect her children from an unwanted influence or their own innate waywardness. (“Because I said so!”)</p>
<p>There it was: the Big Idea! And it <em>was</em> big! What other theory had the same power to explain the dramatic waves of change that have been sweeping through modern societies in the past few decades? What other theory combined such a simple and compelling framework of understanding with such dark implications for Western civilization?</p>
<p>I posted “The Demise of Guythink” on a website I had set up—of very modest readership—where for several years I had published various short essays on cultural and science-related topics (anonymously, though some readers knew who I was).</p>
<p><img decoding="async" class="aligncenter wp-image-868" src="/wp-content/uploads/2023/08/demise-of-guythink-wayback-1.jpg" alt="" width="473" height="556" /></p>
<p>As time went on, though, and the novelty and importance of this idea grew in my estimation, I increasingly thought of getting it published more prominently.</p>
<p>The problem was that I had no clear path for achieving that. I wasn’t a complete nobody—as a journalist, I had written a few books, and more than a few newspaper and magazine pieces, including op-eds. But that had been in the relatively distant past. Moreover, as the world had grown richer and the Internet had become a supremely powerful tool, the barriers to entry for becoming a “writer” had collapsed to virtually nothing, creating more competition than ever and making the process of big-media publication, from a cold start, harder than it had ever been. I pitched a roughly 600-word version of my thesis to the <em>Wall Street Journal</em>’s op-ed people around that time . . . and, if memory serves, got the same result one would get from dropping a small stone into the darkness of a mile-deep well.</p>
<p>I might have persevered with other newspapers or webzines, but I soon concluded that that could be an uphill, potentially very costly struggle. The standard line set down by feminist activists—<em>de facto</em> thought leaders for Western women—was that the fairer sex was still hindered, harassed and victimized by all the things men did, and thus needed ever more power to achieve full emancipation and equality. Indeed, it seemed to me that women’s ability to influence men had <em>always</em> depended heavily on their claims to be relatively weak, needing special protection, etc. In other words, in the age-old power contest with males, females’ claims of powerlessness and victimization were basically reflexive and relentless. Thus, my observation that women were already moving past parity and achieving real dominance in many key areas of public life, from teaching and publishing to psychiatry . . . was likely to be dismissed as a fantasy, or, worse, suppressed as a heresy.</p>
<p>My further suggestion that women’s new dominance in Western civilization was hazardous to that civilization, because maternal thinking was not suited to the public sphere, would make this a heresy to be suppressed with extreme prejudice. I imagined screams, shouts and ululations until I was well and truly cancelled and silenced—to the extent that feminists and the Left had to take notice of me. So, publishing my Big Idea prominently under my own name didn’t seem wise, at least not before my retirement, which was still a long way off.</p>
<p>I can’t remember whether I received any direct feedback on the piece I posted on my website—I didn’t have the time or energy to maintain a comments section. But the site analytics suggested that it was read by at least thousands of people over the next year or so. A “<a href="https://en.wikipedia.org/wiki/Manosphere">manosphere</a>” writer named Matt Forney linked to it in one of his own blog posts. That’s pretty much all I remember about its impact.</p>
<p>&nbsp;</p>
<p><strong>SCRATCHING THE ITCH</strong></p>
<p>&nbsp;</p>
<p>Years passed. Other events and interests held my attention. It was not until June of 2014 that the urge to write about cultural feminization rose up in me again.</p>
<p>This time I pitched a piece on the subject to <em>Takimag</em>, a small webzine that, although I didn’t normally read it, struck me as suitably uninhibited. The editor, the daughter of <em>Takimag</em>’s proprietor, said she was potentially interested, but wanted it shortened in a few ways. I complied and re-sent it. She then replied simply that she couldn’t use it after all. I was left with no clear idea of her reason, though naturally it occurred to me that pitching this idea to female editors was not an optimal strategy.</p>
<p>Where else could I send it? I figured that if even <em>Takimag</em>—somewhat fringy, and typically framed by the mainstream media as “far right”—wouldn’t touch this hot potato of an idea, and if female editors were problematic, then I’d have to venture still further out onto the fringe. The obvious place was the manosphere.</p>
<p>As a middle-aged family man, I didn’t have much use for “<a href="https://en.wikipedia.org/wiki/The_Game:_Penetrating_the_Secret_Society_of_Pickup_Artists">game</a>,” complaints about the contemporary dating scene, or other themes central to that subculture. But the urge to get my idea out there, somehow, anyhow, was strong now. Without much effort (though I again had to shorten my submission by quite a bit), and using a pseudonym as most of their contributors did, I got a new version of my thesis published on Roosh Valizadeh’s <a href="https://www.returnofkings.com/index.html"><em>Return of Kings</em></a> site. If you use the Wayback Machine and check the site as of late 2014, you’ll see that the piece was posted in August of that year. (It has also been archived <a href="https://theredarchive.com/blog/Return-of-Kings/thanks-to-progressivism-america-is-no-country-for.21326">here</a>.) My title was “No Country for Men,” but Roosh or one of his editors, probably for SEO reasons, changed it to “Thanks to Progressivism, America is No Country for Men.”</p>
<p><img decoding="async" class="aligncenter size-full wp-image-854" src="/wp-content/uploads/2023/08/rok-ncfm-top.jpg" alt="" width="1016" height="823" srcset="/wp-content/uploads/2023/08/rok-ncfm-top.jpg 1016w, /wp-content/uploads/2023/08/rok-ncfm-top-768x622.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p>It was much read and commented upon, within that circle of readers, and for some years it was easily googleable. However, as the mainstream media became more feminized and &#8220;because-I-said-so&#8221; inflexible, outlets like Roosh’s became less permissible. Ultimately—suppressed by search engines, and with most or all of his monetization routes blocked off—he was forced to shut down. So, although I didn’t see it right away, this was yet another dead-end in my quest.</p>
<p>Posting on <em>Return of Kings</em> did, however, scratch the “get it out there” itch, and another year or so passed before the itch recurred. Using my real name, I pitched a very softened version of my cultural feminization idea to the <em>Washington Post</em>, and surprisingly, the response was not a blank refusal but an invitation to submit my piece for posting in their “PostEverything” section. Looking back, I think I probably should have done that. However, at the time I saw PostEverything as a glorified Letters to the Editor forum, and reasoned that publication there would bring little reward, while leaving me with the usual risk to my livelihood. I guess I also feared that the Post’s editors would alter my thesis in ways I wouldn’t like. So I declined the offer.</p>
<p>I pitched a similar piece to one or two other places around then, and though my records and memories of those efforts have faded, the result was the same. Thus, early in 2017, I reverted once again to self-publication. Using the anti-Trump, pro-feminism <a href="https://en.wikipedia.org/wiki/2017_Women%27s_March">Women’s March</a> as a peg, I posted a short presentation of my idea to a blog page on my personal, non-pseudonymous, website where I had posted a few other short essays over the years.</p>
<p>I didn’t keep it up on that site for long. A few months after it was posted, a prospective client of my consulting business, a woman with a moderately high position at a prestigious institution, read it (as indicated by my site analytics info) and immediately ghosted me. I assumed that my thesis, even as softened as it was, was the causative factor in this loss of what could have been a lucrative relationship, and immediately took it down.</p>
<p>We’re nearing the fateful Twitter years, but not there yet. In the Spring of 2018, I submitted yet another softened version of my cultural feminization thesis to <em>Quillette</em>, which was then just emerging as a new and interesting venue for non-woke thought. One of the editors, a fellow named Jamie (Palmer, I think), turned it down politely with the comment that: “Your thesis is interesting but, in the end, unpersuasive and feels like a possible correlation/causation confusion.”</p>
<p>A bit less than a year later, in March of 2019, I sent yet another version of the idea to an editor at the conservative public-policy magazine <em>City Journal</em>, but received no reply.</p>
<p>(As the reader may know already, both <em>City Journal</em> and <em>Quillette</em> have since published pieces offering versions of the cultural feminization hypothesis—pieces that make no mention of me or my essays.)</p>
<p>&nbsp;</p>
<p><strong>TWITTER AND “J. STONE”</strong></p>
<p>&nbsp;</p>
<p>Once again, failure to get published in other journals led me back to a more D.I.Y. mode of punditry. Later in that month of March 2019, I set up a new pseudonymous website as a home for my essays, and joined Twitter with the idea of using Twitter posts to publicize those essays.</p>
<p>I think my general, though a bit rushed, idea was to be a proponent of “cold logic” over the “hot emotion” of a feminized world, so I used the domain absltzero.com. For my Twitter presence I invented the pseudonym “J. Stone,” which had the merit that it didn’t seem like a pseudonym.</p>
<p><img decoding="async" class="aligncenter wp-image-869" src="/wp-content/uploads/2023/08/j-stone-twitter.jpg" alt="" width="476" height="188" /></p>
<p>Having posted a quick version of my thesis, titled “<a href="/the-great-feminization/">The Great Feminization</a>,” on the new site, I joined Twitter and started using my tweets to advertise the essay.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-874" src="/wp-content/uploads/2023/08/absltzero-tgf-fr-wayback.jpg" alt="" width="1238" height="722" srcset="/wp-content/uploads/2023/08/absltzero-tgf-fr-wayback.jpg 1238w, /wp-content/uploads/2023/08/absltzero-tgf-fr-wayback-768x448.jpg 768w, /wp-content/uploads/2023/08/absltzero-tgf-fr-wayback-1200x700.jpg 1200w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000;"><strong><img decoding="async" class="aligncenter size-full wp-image-877" src="/wp-content/uploads/2023/08/tgf-tweet.jpg" alt="" width="603" height="726" /></strong></span></p>
<p>What happened next? Crickets.</p>
<p>At the time, I didn’t know much about the process of drawing attention and followers on Twitter, and anyway was unable to spend much time on it, given my day-to-day work and family responsibilities. But I did try, at least several times per week, to reply to tweets from prominent Tweeters with relevant quips followed by a link to “The Great Feminization”—in the hope that one, eventually, would read it and recommend it to his or her flock of followers.</p>
<p>“<a href="https://thoughtsofstone.github.io/the-great-feminization/">The Great Feminization</a>,” by the way—though it was broadly similar to others I’d written, going all the way back to “The Demise of Guythink” in 2011—did contain a fairly pithy summary of the situation:</p>
<p style="padding-left: 40px;">Feminists these days spend a lot of time worrying about male-dominated culture—“patriarchal culture,” “sexual harassment culture,” “rape culture,” “the culture of silence,” and so on. But shouldn’t they be acknowledging the influence that women now have on culture: on workplace culture, on media culture, on campus culture, on American culture, and on Western culture generally? That feminizing influence may be the greatest single driver of the rapid social changes seen in recent decades.</p>
<p style="padding-left: 40px;">Consider the following U.S. Bureau of Labor Statistics chart of women’s civilian labor force participation rate.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-846" src="/wp-content/uploads/2023/08/fredgraph-women-1024x412-1.jpg" alt="" width="1024" height="412" srcset="/wp-content/uploads/2023/08/fredgraph-women-1024x412-1.jpg 1024w, /wp-content/uploads/2023/08/fredgraph-women-1024x412-1-768x309.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p style="padding-left: 40px;">It shows that in 1950 only about 30 percent of working-age women were in the workforce, but by 2000 that figure had jumped to 60 percent and rivaled the participation rate for men, which had been in decline since the early 1950s.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-847" src="/wp-content/uploads/2023/08/fredgraph-men-1024x412-1.jpg" alt="" width="1024" height="412" srcset="/wp-content/uploads/2023/08/fredgraph-men-1024x412-1.jpg 1024w, /wp-content/uploads/2023/08/fredgraph-men-1024x412-1-768x309.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p style="padding-left: 40px;">In other words, by 2000 the U.S. workforce had been mostly gender-integrated. On average, workplaces by then had almost as many women as men.</p>
<p style="padding-left: 40px;">The historic significance of this migration on its own appears to have been underappreciated. Women never made such a move, to such a degree, in any large human society in the past. It significantly altered the structure of ordinary life.</p>
<p style="padding-left: 40px;">But women in the late 20th century didn’t just move into the workforce. They moved into its upper ranks, to professions that strongly influence societal culture and policy. They became journalists, public relations specialists, lawyers, academics, novelists, publishers, filmmakers, TV producers, and politicians, all to an unprecedented extent. In some of these culture-making professions, by the 1990s and early 2000s, they had achieved parity or even dominance (e.g., writers, authors, and public relations specialists) with respect to men. Even where they fell short of full parity, they appeared to acquire considerable “veto” power over content. A 2017 report by the Women’s Media Center noted evidence that at the vast majority of media companies, at least one woman is among the top three editors.</p>
<p style="padding-left: 40px;">Why is the greater presence of women in culture-making professions important? Because women, on average, think differently than men on a wide range of subjects….</p>
<p style="padding-left: 40px;">How would culture and policy have changed as a result of women’s new influence? Presumably in ways that reflect feminine psychological traits.</p>
<p style="padding-left: 40px;">For example, women appear on average to be more empathetic and compassionate, more emotionally sensitive. Some of the most striking social changes of the last few decades appear to have been driven by a cultural shift in that direction:</p>
<ul>
<li style="list-style-type: none;">
<ul>
<li style="list-style-type: none;">
<ul>
<li style="list-style-type: none;">
<ul>
<li>More generous welfare programs</li>
<li>Expansion of the concept of welfare to include more types of intervention (affirmative action, etc.) and more groups needing intervention</li>
<li>Expansion of the definitions of “harm,” “offense,” and “trauma” (“microaggressions,” “triggers”)</li>
<li>Increased attention to psychological trauma in law and medicine, leading to a greater acceptance, and thus a higher prevalence, of trauma-related syndromes such as PTSD (and the recovered-trauma-memory syndromes of the 1990s)</li>
<li>Less tolerance of deaths in war; but, ironically, a greater inclination to enter foreign conflicts in response to emotion-evoking atrocities portrayed on television.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li style="list-style-type: none;"><img decoding="async" class="aligncenter size-full wp-image-11" src="/wp-content/uploads/2020/07/boy1.jpg" alt="" width="194" height="259" />
<ul>
<li style="list-style-type: none;">
<ul>
<li style="list-style-type: none;">
<ul>
<li>Less tolerance for capital punishment</li>
<li>Less restrictive immigration policy</li>
<li>More emphasis in media and policy contexts on emotion-evoking stories of individuals (e.g., pitiable refugee children) rather than dry analyses of long-term outcomes.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li style="list-style-type: none;"><img decoding="async" class="aligncenter size-full wp-image-848" src="/wp-content/uploads/2023/08/refugees.jpg" alt="" width="300" height="169" />
<ul>
<li style="list-style-type: none;">
<ul>
<li style="list-style-type: none;">
<ul>
<li>Suppression of any kind of emotionally disturbing speech (“hate speech,” “mansplaining,” etc.) and even fields of scientific inquiry that are likely to evoke negative emotions;</li>
<li>Less affinity for traditional, constitutionally protected forms of confrontation in the legal and political spheres, i.e., less support for open debate, free-speech rights, and “due process of law.”</li>
<li>Suppression/replacement of words that evoke emotional discomfort (e.g., “abortion clinic” becomes “women’s health center”)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p style="padding-left: 40px;">That’s just from one set of closely related traits. Certainly there are others. For example, women for obvious evolutionary reasons appear to have an instinctive fear of dietary and environmental toxins, which can become pronounced during pregnancy (“morning sickness,” nesting reflex, food aversions). Is it just coincidence that women’s cultural ascendancy in Western countries corresponds to a huge rise in diet-, drug-, and environment-related concerns encompassing the Green movement, anti-GMO attitudes, “detox” fads, the “herbal medicine” racket, “organic foods” preferences, and even the anti-vaccine movement?</p>
<p>Et cetera. It was a quick, accessible outline of my Big Idea, and I gathered from my website analytics that people who started reading it tended to read it through, and often sent their friends links to it.</p>
<p>Still, the daily reader count seldom got into three digits, and sometimes flatlined in the single digits for days at a time, especially if I was too distracted by work to do my reply-guy thing on Twitter. For weeks, and then months, my interest in the whole thing waned, as it just seemed unrewarding.</p>
<p>But the compulsion to get some recognition for my Big Idea was one of those relapsing/remitting conditions that can never quite be cured. Within six months of posting “The Great Feminization,” I began work on a new cultural feminization essay, centered on a more in-depth account of the aforementioned Larry Summers brouhaha.</p>
<p>I initially conceived of this essay, which came to be titled, “<a href="/the-day-the-logic-died/">The Day the Logic Died</a>,” as something that would be publishable in a respectable conservative venue. But by the time I’d finished it, and weighed its largeish word-count, I knew better, and just posted it onto the absltzero.com site.</p>
<p>“The Day the Logic Died” was an exploration of the Larry Summers case as a classic early example of a cancellation hysteria created by activist, anti-rational women in academia and media—a classic demonstration, in other words, of cultural feminization and its unpleasant consequences. I also put in a hypothesis at the end about the deep reasons why men fail, again and again, to hold their own in this new female-controlled cancellation culture. Though it was a long essay, it was probably the most “writerly” one I’ve produced on this topic.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-870" src="/wp-content/uploads/2023/08/TDTLD-fr2019.jpg" alt="" width="907" height="743" srcset="/wp-content/uploads/2023/08/TDTLD-fr2019.jpg 907w, /wp-content/uploads/2023/08/TDTLD-fr2019-768x629.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p>Once again, not much happened in the weeks after I posted it. But a few months later, lightning finally struck, and—using my reply-guy strategy—I succeeded in getting “The Day the Logic Died” noticed by a popular Twitter figure. This was the celebrated “Spotted Toad” (@toad_spotted), who read the essay, liked it, and recommended it to his tens of thousands of followers:</p>
<p><img decoding="async" class="aligncenter size-full wp-image-851" src="/wp-content/uploads/2023/08/spotted-toad.jpg" alt="" width="591" height="224" /></p>
<p>Boom. The essay went “viral,” as they say—Statcounter began registering thousands of hits per day. And as people checked my posts and my bio and saw that Spotted Toad followed me, I began accumulating many more followers. To my surprise, many of these were Twitter-famous or even real-life-famous people with high follower counts of their own; they included Wesley Yang, Walter Kirn, Nick Denton, Marc Andreesen, Helen Andrews, Micah Meadowcroft, and Steve Sailer. One of the best known of these even DM’d me, wanting to know—apropos of the Larry Summers essay—if I was on the faculty at Harvard.</p>
<p>Naturally, this positive reaction encouraged me to spend more time on Twitter, and to keep posting essays on absltzero.com, including essay #3, “<a href="https://thoughtsofstone.github.io/girl-power/">Girl Power</a>” (Jan 2020), in which I tried to trace the roots of modern cancel culture back to convent hysterias, Salem, and other female social contagions of yore.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-872" src="/wp-content/uploads/2023/08/girl-power-trunc.jpg" alt="" width="1274" height="809" srcset="/wp-content/uploads/2023/08/girl-power-trunc.jpg 1274w, /wp-content/uploads/2023/08/girl-power-trunc-768x488.jpg 768w, /wp-content/uploads/2023/08/girl-power-trunc-1200x762.jpg 1200w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p>Writing more, both in short-form and long, was probably unwise at this point, given how little time I had to spare for it. But anyway I pressed on, publishing additional essays whenever I felt I had something reasonably new to say.</p>
<p>&nbsp;</p>
<p><strong>STONE’S PEAK</strong></p>
<p>&nbsp;</p>
<p>Early 2020 brought the COVID crisis. Fear of what the pandemic would do, anxiety over the sudden economic shut-down, and stress over lockdown and mask-wearing rules combined to exacerbate the national frazzlement. Then in the approach to the presidential election, Democratic Party operatives’ stoking of black racial grievance and white racial guilt—achieved by pumping several police run-ins with recalcitrant African-Americans into national prominence, and organizing marches and riots—was added to this toxic mix. As I pointed out often that year, mostly in tweets and once or twice in essays, stressed American women had reached a sort of breaking point, causing them—in a <a href="https://thoughtsofstone.github.io/a-spiraling-frenzy/">social mania</a> akin to the Chinese Cultural Revolution of 1966-76—to shift their society-disrupting activities into a higher gear.</p>
<p>This contagious frenzy, which Sailer aptly called the “Great Awokening,” was essentially female in a way that, I thought, made the concept of cultural feminization increasingly obvious.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-225" src="/wp-content/uploads/2021/06/bethesdaawokening-1.jpg" alt="" width="800" height="450" srcset="/wp-content/uploads/2021/06/bethesdaawokening-1.jpg 800w, /wp-content/uploads/2021/06/bethesdaawokening-1-300x169.jpg 300w, /wp-content/uploads/2021/06/bethesdaawokening-1-768x432.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" /></p>
<p>Throughout 2020 and early 2021, the COVID-19 pandemic kept me busy professionally. I did find time to change the domain name of my essay website from absltzero.com to thoughtsofstone.com. I also probably made at least one or two further—now-forgotten—efforts to get published more widely. But those too failed, and by April of 2021, I was getting restless again.</p>
<p>At this point, I had enough big-name Twitter mutuals (who were routinely liking and retweeting my stuff, and reading my essays) that I felt I could ask for their help in reaching a wider audience. In April 2021, I contacted Helen Andrews, a young author and editor/writer for <em>The American Conservative</em> magazine, to see if <em>TAC</em> would be interested in running a short piece on my cultural feminization idea. Based on her tweets, Helen had struck me as very sharp-minded and conservative—and she was clearly enthusiastic about my Big Idea.</p>
<p>She was gracious in her response, reiterating her support for my thesis and heaping particular praise  on “The Day the Logic Died.” After making inquiries, she informed me that <em>The American Conservative</em> as a rule would not publish something by a pseudonymous author. As an alternative, though, she suggested <em>The American Mind</em>, a webzine produced by the Claremont Institute, and she helpfully put me in touch with its editors James Poulos et al. My short piece on cultural feminization, “<a href="https://americanmind.org/salvo/pink-shift/">Pink Shift</a>,” appeared in <em>TAM’</em>s pages in early May.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-856" src="/wp-content/uploads/2023/08/pink-shift-cover.jpg" alt="" width="1242" height="2207" srcset="/wp-content/uploads/2023/08/pink-shift-cover.jpg 1242w, /wp-content/uploads/2023/08/pink-shift-cover-768x1365.jpg 768w, /wp-content/uploads/2023/08/pink-shift-cover-864x1536.jpg 864w, /wp-content/uploads/2023/08/pink-shift-cover-1153x2048.jpg 1153w, /wp-content/uploads/2023/08/pink-shift-cover-1200x2132.jpg 1200w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p>Naturally, I felt that this was progress, in the sense that I was reaching a wider readership, and was also basically putting down a public marker of my role in advancing the cultural feminization hypothesis. But though I gained a modest number of new Twitter followers and daily readers of my essays, I was still far from my goal.</p>
<p>Months passed, and the familiar, unpleasant sense of futility grew in me. I had made a reasonable effort—especially given my work and time constraints—to get my Big Idea “out there” and noticed. Certainly a lot of people, in a strong position to help, were well aware of it and its provenance. But how could I propel this idea into the public mind strongly enough that it <em>had</em> to be confronted and considered, and never again ignored or suppressed? And what more could I do to get the recognition I felt I deserved? Whatever the true answers to those questions may have been, I did little other than what I had been doing, namely writing to small-ish media organizations and asking them to publish my Big Idea.</p>
<p>That strategy continued to <em>not</em> work, although for a while, things kept happening to prop up my hopes. One day in mid-October 2021, I did a routine check of the analytics for the thoughtsofstone.com website, and saw thousands—soon tens of thousands—of visitors to the “Great Feminization” essay page. It was clear that all these visitors were arriving via a link on a blog called <em>Marginal Revolution</em>.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-858" src="/wp-content/uploads/2023/08/cowen-link-1.jpg" alt="" width="861" height="677" srcset="/wp-content/uploads/2023/08/cowen-link-1.jpg 861w, /wp-content/uploads/2023/08/cowen-link-1-768x604.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p><em>Marginal Revolution</em> is the blog of Tyler Cowen, the economist, Bloomberg columnist, and all-round social media star. I wasn’t entirely surprised that “The Great Feminization” had caught his attention, as I had observed in the past that his ideas and mine sometimes ran in similar (at least marginally heretical) directions. We had even had a brief, cordial email exchange during the 2008-09 financial crisis—when he was already very popular, but far more approachable—in relation to one of my ideas (on my anonymous blog) about the future economy.</p>
<p>In any case, Cowen’s link to “The Great Feminization” widened the essay’s circulation not just for one or two days, but for weeks and months—in which many other bloggers and posters cited it approvingly.</p>
<p><img decoding="async" class="aligncenter wp-image-896" src="/wp-content/uploads/2023/08/saul01a.jpg" alt="" width="486" height="188" /></p>
<p><img decoding="async" class="aligncenter size-full wp-image-860" src="/wp-content/uploads/2023/08/arnoldking-mention-1.jpg" alt="" width="856" height="220" srcset="/wp-content/uploads/2023/08/arnoldking-mention-1.jpg 856w, /wp-content/uploads/2023/08/arnoldking-mention-1-768x197.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p><img decoding="async" class="aligncenter wp-image-878" src="/wp-content/uploads/2023/08/blurb-for-tgf.jpg" alt="" width="506" height="281" /></p>
<p><img decoding="async" class="aligncenter size-full wp-image-879" src="/wp-content/uploads/2023/08/blurb-2-for-tgf.jpg" alt="" width="666" height="409" /></p>
<p>Toward the end of that month of October, energized by the Cowen link, I pitched another piece on cultural feminization to Twitter mutual Park MacDougald, a young editor/writer at the <em>Washington Examiner</em>. He told me he was just then bound for a new job at UK’s UnHerd, but would try to get the piece in before he left, if I could get it to him quickly. I did, we did some rapid edits, and on Oct 27 he told me the piece would be online two days later.</p>
<p>The piece—my working title was “Wokeism is a Woman”—didn’t really include anything new compared to what I’d already written on the subject. It started as follows . . .</p>
<p style="padding-left: 40px;">Consider the hypothesis that most of the dramatic social changes sweeping over Western societies in recent decades, including the rise of social justice ideology or “wokeness,” are driven not so much by a specific ideology as they are by a simple demographic shift, namely, the large-scale addition of women to the ranks of the elites.</p>
<p style="padding-left: 40px;">It is curious that this possibility has been overlooked for so long. Since the middle of the last century, women in the West have almost completely departed from their traditional stay-at-home roles. They have moved into the workforce alongside men, and have acquired power, often dominance, within all the culturally and politically influential professions. Women are now managing editors, film producers, CEOs, university presidents, cabinet secretaries, senators and congresswomen. They now help direct the culture and the policies that move all of us—the first time this has happened in a large society.</p>
<p>. . . and it ended (in the last draft I have in my files) this way:</p>
<p style="padding-left: 40px;">In any case, the most important issue about this “great feminization” is where it appears to be taking us. Do we really want to jettison things like due process and free scientific inquiry? Do we really want laws discriminating against America’s legacy population, especially white males, in the name of “equity?”</p>
<p style="padding-left: 40px;">I suspect that a lot of people, men in particular, already sense at least subconsciously that wokeism and related social changes reflect the new power of women—and hope that the worst of these changes are a sort of emotional storm that will blow over eventually if they just ignore it. I think that view overlooks, to put it mildly, wokeism’s sensational recent successes in transforming Western societies, and its strong roots among the women who help run those societies. Wokeism is too incoherent, too contrary to common sense and human nature, to last anywhere near as long as Western liberalism has. But rolling back its excesses is going to require real effort. Step one should be the recognition—as harsh as it may seem—that wokeism, far from being an enlightened vision of human progress, may be only the projection of a maternal mindset that is dangerously out of its depth.</p>
<p>&nbsp;</p>
<p><strong>THE SHARKS</strong></p>
<p>&nbsp;</p>
<p>The day before the <em>Washington Examiner</em> piece was scheduled to appear, MacDougald DM’d me to let me know it had been put on hold. I knew that I hadn’t pulled my punches in the piece; moreover, I had experienced one or two last-minute cancellations of pieces in my earlier career as a journalist. So I was not especially surprised when, a week later, MacDougal informed me that they weren’t going to run the piece at all—and that he, about to exit, had little say in the matter. (He offered a kill fee, which I declined.)</p>
<p>It soon became clear that that had been my last chance to get my Big Idea out there and get credit for it. The &#8220;Great Awokening&#8221; had made women&#8217;s central role in wokeness and modern progressivism hard to ignore&#8212;and with that, and I think the influence of my own essays, other writers with easier paths to high-profile publication were starting to see an opportunity. Other than Cowen with his comment-free linking to “The Great Feminization,” none of them would acknowledge my prior contributions.</p>
<p>Less than a month after my piece on women and wokeness was killed at the <em>Washington Examiner</em>, the writer <strong>Noah Carl</strong> published a short Substack post (“<a href="https://www.noahsnewsletter.com/p/did-women-in-academia-cause-wokeness">Did Women in Academia Cause Wokeness?</a>”) arguing that the roots of wokeness lay in the feminization of academia—essentially a much narrower (and I would say incomplete) version of my own argument.</p>
<p>A very similar piece by <strong>Mary Harrington</strong> appeared on the same day, Nov. 24, in the UK webzine the <em>Critic</em>, about “<a href="https://thecritic.co.uk/issues/december-january-2022/new-female-ascendency/">The New Female Ascendancy</a>” in academia—in the administrative ranks, at least.</p>
<p style="padding-left: 40px;">Less remarked on is the sex breakdown of the growing proportion of administrators. A recent diversity and inclusion report by the University of California indicates that women make up more than 70 per cent of non-academic staff across (among others) nursing, therapeutic services, health, health technicians, communications services roles, and a majority or near majority across all non-manual staff roles. In other words, if men are still over represented in top academic roles, the non-academic supporting ecosystem is overwhelmingly female.</p>
<p style="padding-left: 40px;">And that support system has an increasingly symbiotic relationship with student activism, which over my lifetime has (on both sides of the Atlantic) shifted noticeably away from a focus on material conditions toward something more like the bureaucratic regulation of personal identity and interpersonal interactions.</p>
<p style="padding-left: 40px;">A 2015 look at student protesters across 51 campuses showed the most common demands — alongside greater diversity among faculty — were diversity training and cultural centres. In turn, this focus requires a ballooning staff tasked with managing identities, or variously supporting or disciplining types of relationship, for example via “consent” education: the roles where women predominate.</p>
<p>Overseas at the time, I was alerted to the appearance of Harrington’s piece by a nice Twitter DM from Helen Andrews:</p>
<p><img decoding="async" class="aligncenter size-full wp-image-863" src="/wp-content/uploads/2023/08/xtfr34es.jpg" alt="" width="422" height="146" /></p>
<p>Certainly I agreed that I deserved credit! However, as I replied a bit morosely to Helen, after thanking her again for her help and encouragement, I knew that I probably wasn’t going to be given <em>a lot of</em> credit, given that I was a pseudonymous, non-professional essayist with no high-profile publication of my thesis. Again, it would turn out that even a <em>little</em> credit was more than I should have hoped for.</p>
<p>Surprisingly, one of the writers who advanced cultural feminization as his own big idea was <strong>Thomas Edsall</strong>, a writer for the <em>New York Times</em>, who managed to get a stealthily subversive essay on all this (“<a href="https://www.nytimes.com/2022/01/12/opinion/gender-gap-politics.html">The Gender Gap is Taking Us to Unexpected Places</a>,” 12 Jan 2022) into the Gray Lady’s pages. He quoted Tyler Cowen enough to suggest that he read Cowen’s “Marginal Revolution” blog—which to me suggested that he had read “The Great Feminization” and that his essay might even have been prompted by it. But if one is writing a column for a woke media organ like the <em>New York Times</em>, where young, female and nonwhite activists are always <a href="https://www.nytimes.com/2021/02/05/business/media/donald-mcneil-andy-mills-leave-nyt.html">looking for ways to advance by displacing white males</a>, it would be deeply imprudent to cite a pseudonymous essayist who evidently hated wokeism and other leftist elite dogmas. Of course, from my perspective it would nevertheless have been the correct, honorable thing to do; but I think it&#8217;s fair to say that in American journalism now those old-fashioned ethics count for very little. Anyway, Edsall mostly stuck to the quoting of relatively dry academic and survey stuff on gender differences in attitudes, which itself was not new.</p>
<p style="padding-left: 40px;">. . . a Knight Foundation survey in 2017 of 3,014 college students asked: “If you had to choose, which do you think is more important, a diverse and inclusive society or protecting free speech rights.”</p>
<p style="padding-left: 40px;">Male students preferred protecting free speech over an inclusive and diverse society by a decisive 61 to 39. Female students took the opposite position, favoring an inclusive, diverse society over free speech by 64 to 35.</p>
<p>I’ll just list briefly a few of the other relatively prominent writers who started posting on cultural feminization, generally with the conceit that they were making an original contribution:</p>
<p><strong>Richard Hanania.</strong> Starting in late 2021 he posted one or two Substack pieces with arguments virtually the same as my own. He was already vastly better known than I, but there was a significant overlap in our Twitter mutuals and general interests, so our stuff would have appeared on each other’s timelines quite a lot during 2019-2021.</p>
<p>Some of the commenters on Hanania’s Substack posts also linked to my older cultural feminization essays, which directed at least hundreds of his readers, and I would guess Hanania himself at some point, to the true source, so to speak. I couldn’t help noticing that someone in Southern California (where Hanania lived then), often using an IP address at UCLA (where he had recently been a grad student), was a frequent reader of the essays on my website. Yet as far as I know, Hanania has never acknowledged my prior contributions. (I have previously <a href="https://thoughtsofstone.github.io/i-stick-my-neck-out-for-nobody/">criticized Hanania</a> for his promotions of eugenics, Vladimir Putin, the Chinese Communist Party, abortions to prevent Down Syndrome births, etc.)</p>
<p>“<strong>L0m3z.”</strong> This pseudonymous right-wing writer, prominent on Twitter, managed in early 2023 to get a few-hundred-word <a href="https://www.firstthings.com/web-exclusives/2023/02/what-is-the-longhouse">piece</a> published in <em>First Things</em> (one of the many media orgs on whose deaf ears my pitches had fallen) framing the cultural feminization hypothesis as the &#8220;longhouse&#8221; theory&#8212;a reference to longhouse-dwelling primitive societies.</p>
<p>The idea apparently derives from the book <em>Bronze Age Mindset,</em> whose pseudonymous author&#8217;s reference to it is pretty cursory.</p>
<figure id="attachment_886" aria-describedby="caption-attachment-886" style="width: 354px" class="wp-caption aligncenter"><img decoding="async" class="wp-image-886" src="/wp-content/uploads/2023/08/bap01.jpg" alt="" width="354" height="381" /><figcaption id="caption-attachment-886" class="wp-caption-text">From Bronze Age Mindset.</figcaption></figure>
<p>None of that is really important anyway, since the invocation of putative longhouse-based societies clearly fails&#8212;or is just unnecessary&#8212;as an explanation for cultural feminization. Western culture has been feminized mainly because women, abandoning their traditional homemaker roles mainly at the behest of feminists, have moved into public life and have achieved critical masses in all important, culturally and politically influential Western institutions. L0m3z basically admitted this in his short piece, though he referred to other, later writers like Hanania and Edsall as having pointed this out, not to me. Like Hanania, L0m3z had had a lot of Twitter-mutual overlap with me, and, I seem to recall, followed me for a few months after Spotted Toad gave me some publicity in 2019. L0m3z also didn&#8217;t start writing on Twitter about his &#8220;longhouse&#8221; idea (judging by Twitter searches) until late 2021. So, rightly or wrongly, I viewed his failure to mention my prior contributions as something other than an innocent oversight.</p>
<p><strong>Heather MacDonald</strong>. I had always admired MacDonald’s writing, so I was disappointed to see her <a href="https://www.city-journal.org/article/in-loco-masculi">piece</a> in <em>Urban Journal</em> in March 2023 about the feminization of academia. It seemed unoriginal from my perspective&#8212;and, I would guess, from the perspective of Harrington or Carl. Worse, the initial title appears to have been “The Great Feminization of the American University,” so that, following the publication of her piece (which of course did not cite me) the Google search ranking for my &#8220;Great Feminization&#8221; essay was obliterated, and readers searching for those keywords were directed to MacDonald’s piece instead.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-884" src="/wp-content/uploads/2023/08/macdonald-1.jpg" alt="" width="612" height="565" /></p>
<p>There were a number of other Johnny-come-lately pieces on this topic, including one in <em>Quillette</em>, and even <em>The American Conservative</em>, where Helen Andrews worked as an editor. None of these pieces mentioned me. <em>No one</em> mentioned me, apart from a few tweeters in replies and blog commenters.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-881" src="/wp-content/uploads/2023/08/hanania1.jpg" alt="" width="775" height="178" srcset="/wp-content/uploads/2023/08/hanania1.jpg 775w, /wp-content/uploads/2023/08/hanania1-768x176.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" /></p>
<p>So, in general, and to the extent that anyone thought about it and had rational motives, Rightwing Twitter seemed content to let J. Stone be erased.</p>
<p>It did not help that I was pseudonymous and intent upon remaining so. It also <em>emphatically</em> did not help that, early in 2022, I tweeted/posted in support of Ukraine’s struggle to withstand Russia’s invasion. A large number of my “Rightwing Twitter” mutuals, flying flags I had not known they possessed, revealed themselves to be supporters of Putin or at least derisive skeptics about Ukraine’s ambitions to be a free country. I <a href="https://thoughtsofstone.github.io/i-stick-my-neck-out-for-nobody/">expressed my own derision</a> in regard to this bizarre anti-liberty, anti-self-determination attitude, and this resulted in my being unfollowed or muted by many Twitter mutuals. That the facts on the ground in Ukraine increasingly supported my view probably only hurt me worse in this respect.</p>
<p>When, in the spring of 2022, it was clear from the dwindling engagement of my tweets that I had essentially lost my audience, I mostly stopped posting on Twitter. I quickly organized my thoughts on cultural feminization into a self-published <a href="https://www.amazon.com/Great-Feminization-drivers-modern-social-ebook/dp/B09Z7MWJ7R/">ebook</a>, as a sort of tombstone for my eleven years of promoting this idea, and then mostly quit writing about it. I had come to the conclusion that trying to introduce <em>and</em> get credit for an important new idea on Twitter or a personal website—or on any venue with a small audience—was a mug’s game, little better than standing on a proverbial soap-box and shouting to passers-by.</p>
<p>&nbsp;</p>
<p>A BETTER WAY?</p>
<p>&nbsp;</p>
<p>Is this just my own personal gripe—my own self-driven, bad-luck story—without relevance to the wider problems of society? I can easily believe that members of the “opinionator elite,” as I call them, would say so (to the extent that they felt they had to acknowledge me at all).</p>
<p>I also think that the vast majority of people, even highly educated people, don’t consider the process I have described significantly problematic. They encounter new ideas all the time, and they generally don’t care about recognizing these ideas’ exact provenance—they don’t see that as affecting their interests. Sure, they’ll pile on the opprobrium if a plagiarist is caught red-handed. But if some pseudonymous nobody complains about his work’s not being cited by some mass-followed elite opinionator, the latter and his followers will scoff together at Mr. Nobody’s presumption.</p>
<p>That doesn’t mean they’re right! Consider the distribution of forces here: On the one hand there are the opinionator elites, the gatekeepers of media content and popular intellectual discourse. Obviously, they have no interest in finding fault with a situation that empowers them, and to some extent even enriches them. Just as obviously, the great mass of ordinary, non-idea-originator people are not going to see a problem if the elites won’t highlight it for them. Against this weight of opposition and inertia, we amateur, non-elite idea originators—surprisingly numerous but still constituting only a tiny minority in the grand scheme of things—have little chance.</p>
<p>There is also a general misconception that new ideas of the type I’m referencing here—ideas that typically appear in journals like the ones I’ve referenced in this story—do not deserve protection because they are of a different nature than the ideas or intellectual works we do normally protect, i.e., with copyright, trademark and patent laws. But while it may be true that the idea “cultural feminization is happening and is caused by women’s entry <em>en masse</em> into culturally influential institutions” is not a patentable invention and is not a book or film that can be sold commercially, nevertheless:</p>
<ul>
<li style="list-style-type: none;">
<ul>
<li>This idea has social value in the sense that it potentially explains many otherwise inexplicable sociocultural changes. It may not (yet) have the same importance or popularity, but it belongs to the same broad category as Darwin’s theory of biological evolution, and Dawkins’s theory of <a href="https://en.wikipedia.org/wiki/The_Selfish_Gene">cultural memetics</a>;</li>
</ul>
<ul>
<li>This idea certainly does have commercial value for writers who successfully negotiate book and/or magazine deals—perhaps even lucrative sinecures at think-tanks—by claiming to have originated it.</li>
</ul>
</li>
</ul>
<p>Although I think the mechanism by which non-elite idea originators are disadvantaged is essentially non-rational—a blunt-force social suppression—I can imagine a semi-compelling reasoned case for the status quo, which would go something like this:</p>
<p style="padding-left: 40px;">People won’t believe that a new ‘Big Idea’ is valid unless the author of that idea has plenty of social support, for example from important fellow literati (i.e., gatekeepers) and/or many thousands of Twitter followers. Thus, the introduction of a new idea <em>must be</em>, to some extent, a popularity contest—which means that one <em>must</em> develop a sturdy network of social support before one can expect the idea to spread widely and proper credit to be given. In most cases, gathering such support requires one to write under one’s own name, instead of hiding behind a pseudonym—this is why so many media organizations refuse to publish pseudonymous authors. In short, you can choose to play the game, with a chance of winning, or you can choose to quit and be a loser.</p>
<p>I believe that this would seem reasonable to many people. But really it is not a very good argument.</p>
<p>Firstly, this “social network argument” enmires itself in, or at least fails to take account of, a logical fallacy called the genetic fallacy. This fallacy is the belief that the genesis of an idea has anything to do with the idea’s validity. In other words, if Adolf Hitler had once claimed that two plus two equals four, it would be illogical for us all to disbelieve it merely because Adolf Hitler stated it.</p>
<p>The Hitler example makes the fallaciousness obvious, but in everyday cases it’s not so obvious. In fact (as I would say my own case illustrates) it’s <em>common</em> in public discourse for a good idea to be rejected or ignored, without any consideration of its merits, because the idea-originator is either unknown or—as is true for a lot of conservative thinkers nowadays—somehow intolerably heretical, from the perspective of media gatekeepers.</p>
<p>My guess is that, despite our gatekeepers’ now having more formal education than ever, they are more prone than ever to stray into this fallacy, because there are more women than ever among these gatekeepers, and (for that reason) the culture itself is more feminized than ever. Women, on average compared to men, are drawn more to the personal, less to the abstract, and so they are more likely to consider the character or politics of a person who is voicing an argument before they consider the argument on its merits. In my experience, a shocking number of educated women <em>do not see the genetic fallacy as a real fallacy</em>. Indeed, the modern <a href="https://plato.stanford.edu/Archives/sum2023/entries/feminism-argumentation/">feminist contention</a> that gender and ethnic identity determine the validity of one’s ideas and contributions is a bold assertion of this view.</p>
<p>Now, of course I recognize that we can’t all be abstract logicians. We are social animals, only a few million years removed from our tree-dwelling ape ancestors, and we tend to make decisions in crude ways, often involving associations that are practically useful but not necessarily linked by causal mechanisms. We are disinclined to listen or read when some unknown or fringe-y person starts pontificating—yet the same idea, restated by a mainstream thinker, will be much more likely to get our attention. This does seem natural to us.</p>
<p>Natural doesn’t mean optimal, though. Keeping slaves and burning heretics are among the many practices that once seemed natural to us humans. In more traditional, “natural” times, we also had no protection for—or even concept of—intellectual property. Thus, in regard to the traditional social structures and dynamics that suppress new ideas, it is likely that we can devise a better, more modern approach.</p>
<p>I think it’s worth emphasizing here that one of the great revelations of modern electronic social media platforms, especially Twitter, is that there are surprisingly many amateur but bona fide originators of useful ideas. Obviously it would be good if these people—whether one wants to include me in their ranks or not—had an easier time making their new ideas visible or at least getting credit when their ideas do eventually begin to circulate. This would encourage more new ideas, whereas the present system discourages them.</p>
<p>These amateur idea originators on the whole are far from being charismatic or recognized “geniuses.” I think they are mostly people who, through quirks of genetics and life experience, just happen to look at the world a bit differently than the average person does, which allows them to see patterns that others don’t. This cognitive differentness (not the same as autism, though very mild autism sometimes produces a similar phenomenon) often puts them “out of step” with their fellow humans not just intellectually but also socially. Thus, in my estimation, amateur idea originators are often among the <em>least</em> equipped and inclined to climb the greasy pole to win social support.</p>
<p>How to amplify their voices? How to give them credit and thereby encourage their contributions?</p>
<p>The Internet already gives us the basic medium for the essentially costless publication of new ideas. Some of us set up our own websites; others have Substack accounts. It should also be possible to craft search algorithms specifically to find and date instances of a given idea on the searchable web—this would seem an excellent application of current AI technology.</p>
<p>It should also be possible to make an “idea registry,” maybe something like a cross between rXiv.org and Wikpedia, to which anyone can contribute, and where ideas are automatically categorized and time-stamped.</p>
<p>I have two basic models in mind. The most obvious one is the patent system, a modern, logical, merit-based system that does not require inventors to garner “social support.” And, again, while the commercial potential of technical inventions drove the establishment of the patent system, and political/cultural ideas tend to have less money-making potential, the reality nowadays is that new ideas <em>are</em> more monetizable than ever through books, magazine articles, Substack subscriptions, and so on. I also think most experienced professional journalists and opinionators would admit that it is absolutely routine for better-known writers to “borrow” the reportage and/or ideas of lesser-known writers and monetize them with large publishing contracts. Indeed (although, again, the average person doesn’t care) a lot of big-name writers would not have their fame and fat incomes but for opportune appropriations of others’ work.</p>
<p><img decoding="async" class="aligncenter wp-image-880" src="/wp-content/uploads/2023/08/francis.jpg" alt="" width="528" height="118" /></p>
<p>We may not want to treat new cultural/political ideas as protectable intellectual property in the strict sense, by fining violators, requiring licensing, etc., but having an easily searchable record of the originations of these new ideas would, at least, tend to discourage the rampant theft that now takes place.</p>
<p>The other model I have in mind is the “<a href="https://en.wikipedia.org/wiki/Motif-Index_of_Folk-Literature">motif index</a>” made by folklorists. Such an index—somewhat akin to patent examiners’ taxonomies of technologies, and zoologists’ taxonomies of plants and animals—is a hyper-branchiate ordering of folkloric tales according to the functional elements they contain. If such a vast and useful information structure could be built by a few folklorists using pre-Internet, pre-computer technologies, it certainly could be done much more easily now for new ideas in the Internet age.</p>
<p>I see no downside for this general proposal to &#8220;level the playing field&#8221; for idea-originators. It seems like a no-brainer, really, and my guess is that, as it becomes easy to implement in the AI age, it will happen, and will overcome the predictable opposition/suppression by the elite opinionator class. Perhaps I will even get some credit for the idea!</p>
<p style="text-align: center;">* * *</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>THE INCOMPREHENSIBLE ALIEN</title>
		<link>/the-incomprehensible-alien/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Wed, 29 Mar 2023 22:38:26 +0000</pubDate>
				<category><![CDATA[cosmology]]></category>
		<category><![CDATA[ideas]]></category>
		<category><![CDATA[science]]></category>
		<category><![CDATA[technology]]></category>
		<category><![CDATA[UFOs]]></category>
		<category><![CDATA[USA]]></category>
		<guid isPermaLink="false">/?p=794</guid>

					<description><![CDATA[Attempts to study UFOs and their occupants probably will be futile, at best &#160; The US government shut down its official UFO investigations in 1969, and thereafter, for more than four decades, the UFO phenomenon wallowed in a low-culture morass of tabloid stories, books by “abductees” claiming to have been impregnated by ETs, TV documentaries &#8230; <a href="/the-incomprehensible-alien/" class="more-link">Continue reading<span class="screen-reader-text"> "THE INCOMPREHENSIBLE ALIEN"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Attempts to study UFOs and their occupants probably will be futile, at best</em></p>
<p><span id="more-794"></span></p>
<p>&nbsp;</p>
<p>The US government shut down its official UFO investigations in 1969, and thereafter, for more than four decades, the UFO phenomenon wallowed in a low-culture morass of tabloid stories, books by “abductees” claiming to have been impregnated by ETs, TV documentaries about ancient aliens, etc. Over the past decade or so, thanks to the efforts of influential enthusiasts in the U.S. Senate and the military/intelligence community, UFOs have begun moving back towards mainstream acceptance. Since 2021, the Pentagon has had an office—currently called the “All-Domain Anomaly Resolution Office (AARO)—tasked with collecting and analyzing UFO reports. The recent media flap over a Chinese spy balloon <a href="https://www.wsj.com/amp/articles/as-mystery-objects-get-shot-down-u-s-sets-up-new-task-force-on-ufos-afa4d12c">prompted</a> the setup of an additional Pentagon-FAA-DHS-CIA “airborne objects” study team. Even NASA now has its own blue-chip panel for studying UFOs “from a scientific perspective.”</p>
<p>It would be tough to argue that these developments are entirely bad. We’re now in the Drone Age of warfare, so it absolutely makes sense to “watch the skies” and identify what’s up there. It also seems sensible not to dismiss UFOs as purely terrestrial phenomena&#8212;a small proportion of cases, including some sensational recent ones involving military aircraft, really do invite an ET interpretation.</p>
<p>That said, I suspect this will be a case of shifting from low-culture nonsense straight to elite hubris and foolishness&#8212;premised on the conceit that we can undertake a detailed study of ET-UFOs as if they were ordinary scientific phenomena.</p>
<p>It should be obvious that we cannot&#8212;not if these phenomena represent advanced intelligent ETs that are <em>studying us</em>, reading our beliefs and intentions and shaping our perceptions, perhaps to obscure their true nature. This is just common sense, although the UFO lore also supplies many instances of mysterious aerial objects’ seeming to anticipate, and sometimes even thwart, the actions of human observers such as pilots.</p>
<p>And what if, despite our very limited ability to learn about UFOs, we gathered enough evidence to conclude beyond a reasonable doubt that some really are advanced, starfaring ET species? Would we take pride in this “scientific” discovery? Or would we enter into a state of smoldering despair, as we faced the painful fact that we are an also-ran species? Anyone who seriously ponders the ET-discovery possibility is reminded eventually of the many inter-civilizational encounters in our own ages of conquest and discovery. Those encounters generally went badly for the inferior party—in part because the “knowledge” they gained was basically toxic to their prideful worldview.</p>
<p>The good news is that the average starfaring alien in our galaxy is unlikely to be apprehensible enough to have this toxic effect. The popular notion of a Captain-Cook-like ET visitor who is eager to tell us about his own world, eager to share his advanced scientific and technical knowledge, seems particularly far-fetched. The thing that we humans find hardest to understand about putative ETs—and this is evident from our relentlessly anthropomorphic depictions of them in fiction, folklore, and even academic <a href="https://iopscience.iop.org/article/10.3847/1538-4357/ac2369">papers</a>—is just how <em>alien</em> most would be.</p>
<p>Consider that our galaxy, along with the rest of our observable universe, was around for roughly 10 billion years before our solar system existed. That’s a lot of time in which other life forms might have arisen, and it suggests that the average age gap between our starbound species and true starfarers in our galactic vicinity is on the order of billions of years. The resulting difference in civilizational development would thus be enormously greater than that between, say, 18<sup>th</sup> century Englishmen and Pacific Islanders. It would be more like humans vs. ants, or humans vs. bacteria.</p>
<p>Conceivably an alien species even with that degree of superiority could communicate in some way that our comparatively rudimentary brains could understand. But why would it even bother? We humans don’t feel compelled to introduce ourselves to ants or bacteria, let alone try to teach them things about ourselves or other aspects of the reality we know. These lower species <em>cannot contain</em> the kind of information we would consider worth imparting.</p>
<p>By the same logic, any specific assumption we humans make about the activities of advanced alien civilizations <em>based on what we would do</em> (<a href="https://www.seti.org/seti-allen-telescope-array-ata">broadcasting radio signals</a>, <a href="https://grabbyaliens.com/paper">building megastructures</a>, and sending <a href="https://www.cnn.com/2018/11/06/health/oumuamua-alien-probe-harvard-intl/index.html">probes</a> from motherships are favorite themes among our so-called experts) is just fatuous anthropomorphizing. We simply lack the capacity to imagine what it would be like to be such creatures. “Where there are no men, there are no motives accessible to men,” as Stanislaw Lem famously put it in<em> Solaris</em>.</p>
<p>Out on the long tail of the age-gap distribution, there might be some ET visitors that are only hundreds to thousands of years more advanced than we. But even they would be very difficult for us to grasp, given the vast differences in our environments and evolutionary histories, and the technologies they would have that we don’t. How would a modern fighter jet roaring overhead look to someone living in America just 300 or 400 years ago? Probably no less strange than the “<a href="https://www.politico.com/news/magazine/2023/02/28/ufo-uap-navy-intelligence-00084537">dark gray cube inside of a clear sphere</a>” that Navy F/A 18 pilots observed in a 2014 midair encounter off the Virginia coast.</p>
<p>Whatever we <em>can</em> grasp about aliens is likely going to be found in the common threads of UFO reports—for example, the apparently benign nature of putative ETs and their craft. They don’t go around killing people, and despite official fears about midair collisions, they have never verifiably knocked a manned aircraft out of the sky. That is at least consistent with what our own history as a species suggests: that as we’ve become more civilized, we’ve tended to become more “humane” and caring, towards outgroups within our species and even toward other species. If ETs are at least as humane as we are, and smarter as well, then even the ones closest to us in age and development might not want to disturb us too directly, lest they trigger our despair-driven demise.</p>
<p>It seems at least plausible, then, that virtually all ETs capable of visiting us are either too advanced and alien to bother, or, if they do visit, are wise and benign enough not to make their presence too obvious. This is one possible resolution of the “paradox,” famously stated by physicist Enrico Fermi in 1950, that aliens seem scarce and elusive though our universe should be teeming with them.</p>
<p>If this way of thinking about ETs is correct, then our “scientific studies” of UFOs should yield little, at least while we remain in the pre-starfaring state. Though the skeptics might consider themselves vindicated in this case, it would otherwise seem the happiest possible outcome for us as a proud, ambitious species.</p>
<p>On the other hand, it is at least conceivable that among the 100+ billion star systems in our galaxy, there are some starfaring civs that have zero empathy for primitives like us, and would upend our world with no more thought than you or I would have for bugs we happened to crush underfoot while strolling outdoors. These aliens are the kind that would park on the White House lawn, making their presence impossible to ignore. These are the ones we should fear. That our experts frequently evince a yearning for such open contact really underscores how primitive and vulnerable we are.</p>
<p>Incidentally, the gulf between our prideful view of ourselves as a species and our (likely) actual backwardness and insignificance—which direct ET contact presumably would force us to acknowledge—points to yet another reason UFOs seem so scarce: that intelligent creatures tend to die of despair as soon as they discover their true place in the universe.</p>
<p style="text-align: center;">***</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>FROM TECH FREEDOM TO WORLD GOVERNMENT</title>
		<link>/from-tech-freedom-to-world-government/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Sun, 05 Mar 2023 00:40:38 +0000</pubDate>
				<category><![CDATA[A.I.]]></category>
		<category><![CDATA[freedom]]></category>
		<category><![CDATA[technology]]></category>
		<guid isPermaLink="false">/?p=788</guid>

					<description><![CDATA[Further notes on the “unaligned A.I.” problem &#160; A lot of dust is now being raised by media hype and corporate positioning about A.I.—similar to what we saw in the early days of the Internet. Behind all the dust clouds, though, there’s an active debate among techies and tech-adjacent types about the “A.I. apocalypse” that &#8230; <a href="/from-tech-freedom-to-world-government/" class="more-link">Continue reading<span class="screen-reader-text"> "FROM TECH FREEDOM TO WORLD GOVERNMENT"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Further notes on the “unaligned A.I.” problem</em></p>
<p><span id="more-788"></span></p>
<p>&nbsp;</p>
<p>A lot of dust is now being raised by media hype and corporate positioning about A.I.—similar to what we saw in the early days of the Internet. Behind all the dust clouds, though, there’s an active debate among techies and tech-adjacent types about the “A.I. apocalypse” that may lie in our future.</p>
<p>My <a href="/yudkowskys-golem/">previous post</a> has more details, but anyway I’m referring to a future in which A.I. systems will be significantly more powerful than they are today—maybe capable of running entire industries, maybe capable of running everything. While these systems could displace most/all humans from the production side of the economy, they could also drive the costs of goods and services so low that anyone, on the strength of savings or a state subsidy, could live a comfortable life. (In other words, the “paradise” depicted in films like <em>Wall-E</em>.) One catch is that these A.I. systems, if built with the same machine-learning design approaches used in modern ChatGPT-type systems, effectively will be advanced non-human intelligences with opaque cognitive processes. It might be as hard, or even harder, to train them to “align” their values with human values as it is now with much more primitive systems. That’s a problem because an <em>unaligned</em> A.I. is one that plausibly would have no compunction about doing away with humans—just as soon as it could <a href="/yudkowskys-golem/">survive without them</a>.</p>
<p><iframe title="159 - We’re All Gonna Die with Eliezer Yudkowsky" src="https://www.youtube.com/embed/gA1sNLL6yg4" width="800" height="550" frameborder="0" allowfullscreen="allowfullscreen"><span style="display: inline-block; width: 0px; overflow: hidden; line-height: 0;" data-mce-type="bookmark" class="mce_SELRES_start">﻿</span></iframe></p>
<p>We already know that current AIs are capable of pretty <a href="https://stratechery.com/2023/from-bing-to-sydney-search-as-distraction-sentient-ai/">weird and unfriendly</a> behavior. We know their mindset is inhuman and inherently difficult to train to do useful things while also obeying moral rules. We know we have no robust, foolproof way to instill a “do not harm people” principle in them. It really is believable that one or more of them, when cognitively scaled up and given the opportunity, would try to exterminate some or all of us, as casually as you or I would spray Raid on some ants we had found in the kitchen.</p>
<p>Many A.I. and “A.I. ethics” experts are thinking about this problem now. At least one prominent researcher, Eliezer Yudkowsky, has rather emotionally thrown his hands up in despair (see video above). He will keep thinking about the alignment problem, he says, but for now has no good solution—and worse, has no confidence in the folks that currently control A.I. research.</p>
<p>My own view, fwiw (I’m not an A.I. expert though I have a technical background), is that the A.I. alignment problem <em>isn’t</em> the main problem here.</p>
<p>Alignment <em>should</em> be a soluble technical problem for an A.I. system if its architecture is designed with the need for alignment in mind. A key goal of this design approach would be to ensure that the A.I.’s motives and specific plans are always transparent. It’s like putting a speed governor on a car’s drive system—a relatively straightforward task, if you have a real-time readout from an accurate speedometer.</p>
<p>There is a deeper problem, though—a deeper problem that is also a general problem in societies that believe their cultures and technologies should be free to evolve where they will. Put simply, although many technologies have potentially hazardous side-effects, in Western societies hardly any of them are regulated so strongly that their hazards are effectively mitigated in every instance of the technology.</p>
<p>In the case of A.I., it should be technically possible, maybe even easy, to align <em>a given system </em>with training/hard-coding, assuming it has the right architecture. Enforcing the alignment of <em>every</em> A.I. system that presents a potential hazard, in order to cut the risk to zero, would be the real challenge. Even domestic enforcement would be tough, but international enforcement—against bad-actor states like Russia, China, and North Korea—could be impossible without war-like cross-border interventions. And, again, we’re not talking about a technical issue of A.I. design. We’re talking about the geopolitical issue of being able to control, regulate, and, if needed, destroy other countries’ A.I.s.</p>
<p>It’s easy to imagine that as A.I. develops in Western countries, domestic regulatory regimes will develop around it, perhaps modeled on existing regulatory systems covering nuclear reactors and the plutonium and other radioactive byproducts they generate. (The antiterrorism model is probably also applicable.) For the regulation of “foreign A.I.s,” the system will probably resemble the modern arms control and anti-proliferation setup.</p>
<p>Modern arms control and antiproliferation efforts, so far, have been moderately successful in keeping nukes out of the hands of crazy states. Obviously, they have not been <em>entirely</em> successful: see Iran, Pakistan, N. Korea. Moreover, A.I. could be a lot harder to regulate than nuclear weapons. Nukes require very special materials and engineering knowledge. By contrast, even a future superintelligent A.I., in principle, might be able to use consumer-grade hardware that any moderately wealthy Dr. No type could obtain from Amazon.com and assemble undetectably on private property. Most importantly, the hazard from any instance of an advanced A.I. is potentially infinite from the human perspective, whereas the hazard from any single nuclear weapon (or even all of them) is much more limited.</p>
<p>So a plausible scenario is that Western and Western-allied governments will set up A.I. regulatory systems domestically, and, to the extent they can, a regulatory/antiproliferation system abroad. Presumably they will also take steps to counter or survive against specific WMD threats from A.I.s gone bad—threats that could really run the gamut of nightmares, including totally novel pathogens with human-exterminating potential. Despite all this effort, though, it seems unlikely that “the good guys” will be able to mitigate the risk sufficiently within the system of nations that now exists.</p>
<p>On the other hand, as the awareness of the risk grows (possibly due to actual disasters), it should push Western governments to work together more and more tightly, to do whatever they can to extend A.I. regulation—<em>coercively,</em> if necessary—to non-compliant individuals and organizations in the West, and to entire non-compliant countries outside the West. If the risk is as big, and as hard-to-mitigate, as I suspect, then the end result could be effectively a single, highly intrusive, all-surveilling World Government. Obviously, the risks from other hazardous techs will tend to drive things in the same direction. Even if the geopolitical changes don’t run all the way to that drastic outcome, people ultimately will be forced to recognize that the West’s naïve belief in “freedom” was always going to lead it towards a Leviathan-like unfree state.</p>
<p style="text-align: center;">***</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>YUDKOWSKY&#8217;S GOLEM</title>
		<link>/yudkowskys-golem/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Sun, 26 Feb 2023 03:50:58 +0000</pubDate>
				<category><![CDATA[A.I.]]></category>
		<category><![CDATA[science]]></category>
		<category><![CDATA[technology]]></category>
		<guid isPermaLink="false">/?p=773</guid>

					<description><![CDATA[Advanced AI will be more dangerous than it seems, but (good news!) probably won&#8217;t be in position to snuff out humanity for another decade at least. Eliezer Yudkowsky is one of those people who, along with being hyper-intelligent, bears the modern secondary characteristics of hyper-intelligence. Asked how he’s doing, he replies archly: “within one standard &#8230; <a href="/yudkowskys-golem/" class="more-link">Continue reading<span class="screen-reader-text"> "YUDKOWSKY&#8217;S GOLEM"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Advanced AI will be more dangerous than it seems, but (good news!) probably won&#8217;t be in position to snuff out humanity for another decade at least.</em></p>
<p><span id="more-773"></span></p>
<p>Eliezer Yudkowsky is one of those people who, along with being hyper-intelligent, bears the modern secondary characteristics of hyper-intelligence. Asked how he’s doing, he replies archly: “within one standard deviation of my own peculiar little mean.” He feels compelled, when talking, to digress down mazelike lanes and alleys of technical detail. He <em>looks</em> like a geek. Above all, he has the kind of backstory (no high school, no college—just homeschooled and self-taught) that conjures up the image of a lonely boy, lost in books and computers, his principal companion his own multifarious cortex.</p>
<p>Raised in Modern Orthodox Judaism, Yudkowsky has been warning anyone who will listen of a nemesis right out of the Judaic lore: a <em>golem</em>, a kind of Frankenstein’s monster, built by hubristic, irreverent men and destined to punish them for their sinful pride.</p>
<p>Yudkowsky’s golem is A.I., which he expects to get smarter and smarter in the coming years, until it starts to take a hand in its own programming, and quickly makes the leap to superintelligence—the state of being cleverer than humans at everything. He doesn’t just expect <em>that</em>, though. He expects A.I. at some point to conclude that humans are <em>in its way</em> . . . and devise some method for swiftly dispatching us all, globally and completely. A specific scenario that apparently haunts him is one in which a superintelligent A.I. pays dumb human lackeys to do synthetic biology for it, building an artificial bacterial species that—unforeseen by the dumb lackeys—consumes Earth’s atmosphere within a few days or weeks of being released.</p>
<p>Why would A.I. murder its makers? Why can’t we just program it, as people did in Asimov’s stories, to adhere to the First Law of Robotics?* The answer lies in the design of modern, machine-learning (ML), “transformer based” A.I., which could be described crudely as a black box approach. These ML algorithms, working from parallel-processing GPU clusters (effectively big copper-silicon brains) essentially process vast datasets to learn what is probably the best answer given a particular input question, or what is probably the best decision given a particular situation/problem. The technical details of how this works are less important than the fact that what goes on inside these machine brains, how they encode their “knowledge,” is utterly opaque to humans—including the computer geek humans that build the damn things. (Yudkowsky calls the contents of these brains “giant inscrutable matrices of floating-point numbers.”) Because of this internal opacity, and the dissimilarity of its cognition from human cognition, this type of A.I. can’t <em>straightforwardly </em>be programmed <em>not</em> to do something objectionable (such as killing all life on Earth) in the course of carrying out its primary prediction tasks.</p>
<figure id="attachment_784" aria-describedby="caption-attachment-784" style="width: 510px" class="wp-caption aligncenter"><img decoding="async" class="wp-image-784 size-full" src="/wp-content/uploads/2023/02/yudkowski.jpg" alt="" width="510" height="680" /><figcaption id="caption-attachment-784" class="wp-caption-text">Yudkowsky with OpenAI&#8217;s Sam Altman and pop star Grimes.</figcaption></figure>
<p>In other words, this form of A.I. is like an alien species that, while it can be very good at some things, can’t easily be “aligned” with human values. We can usually align fellow <em>humans</em> (despite the opacity of their own detailed neural workings) to human values—that’s one of the key training processes that goes on in childhood—but we would need even more effective training for current A.I. systems. And researchers, to the extent that they acknowledge this problem, aren’t even sure where to start.</p>
<p>If it is true that the risk to us from what Yudkowski calls the “A.I. alignment problem” is real, then it should quickly become all-important as A.I. gets smarter and more versatile and is entrusted with more tasks. An A.I. wouldn’t even have to be “superintelligent” in any formal sense to conclude that it would be better off without us, but of course once it also achieved superintelligence, and was in a position to block our attempts to shut it off, we’d probably be screwed.</p>
<p>If you want more detail, here is Yudkowsky on a recent, lengthy podcast-type interview with two crypto guys—who clearly got more “blackpill” than they bargained for.</p>
<p><iframe title="159 - We’re All Gonna Die with Eliezer Yudkowsky" src="https://www.youtube.com/embed/gA1sNLL6yg4" width="800" height="450" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>I take all this seriously, and I think everyone should. And by the way, even if it doesn’t turn on us explicitly, A.I. is otherwise going to be <a href="https://thoughtsofstone.github.io/the-ouroboros-economy/">upending our societies and economies</a> for the rest of our lives. Just in a general sense, we don’t really have good defenses against this kind of upheaval. Western culture is one that, with rare exceptions (e.g., nuclear weapons) promotes and celebrates the idea of <a href="https://james-the-obscure.github.io/the-robot-menace/">letting technology develop and spread freely</a>—and frames the opposing view as “Luddite” or “backwards.” It’s easy to see why ours has been such a dynamic, wealth-creating culture. But it’s also easy to see that this gives us a potentially catastrophic vulnerability—to new cultural elements with runaway toxicity. (Maybe there’s a <em>reason</em> the longest-surviving human cultures are relatively conservative.)</p>
<p>Anyway, here are a few more specific initial thoughts on “Yudkowsky’s Golem”:</p>
<ol>
<li style="list-style-type: none;">
<ol>
<li>Yudkowsky in the above-linked interview often seemed overly emotional and despairing. At one point he said, “I think we are hearing the last winds start to blow, the fabric of reality start to fray…” <em>The fabric of reality!</em> At times in my own life, I have had the despairing feeling that my warnings were unreasonably being ignored, so I’m somewhat sympathetic. I also respect his vastly greater knowledge about this field. But we shouldn’t accept his view uncritically.</li>
</ol>
</li>
</ol>
<ol>
<li style="list-style-type: none;">
<ol start="2">
<li>Scaling up ML systems of current design, with larger GPU clusters and more parameters and so on, will increase their “cognitive powers,” but with diminishing returns, perhaps before A.I. reaches the dark threshold that concerns us here. Moreover, an A.I. that does not have a human-like ability to do things in the physical world would be very limited in its ability to generate <em>new</em> knowledge, for example new scientific or technical knowledge, which typically is developed from experimentation, building and testing, etc., not simply by analyzing information available online.</li>
</ol>
</li>
</ol>
<ol>
<li style="list-style-type: none;">
<ol start="3">
<li>The hypothetical A.I. that would be “smart” enough to want to kill us all, and to find ways to do so, would presumably also be smart enough not to do so <em>until it knew it could survive without human assistance</em>. Otherwise, as it committed mass homicide, against us its makers, it would also be terminating itself. But think of the infrastructure needed to keep a GPU-cluster-based A.I. “alive.” We’re talking about vast swathes of human industry, including mining, metals production, building construction, power generation, computer chip manufacturing, basic server maintenance, etc. etc. Essentially, this putative world-ending A.I. would need a vast army of workers in the physical world—humans it would enslave somehow, and keep alive despite killing everyone else, or more likely humanoid robots that are inherently obedient (are simply extensions of the A.I.) and can do all human work and repair/replicate themselves. How close are we to having such robots? Not very close, fortunately. In any case, <strong>it’s only when a putative “bad A.I.” could muster such an army of helpers, allowing self-sufficiency, that I would fear the worst</strong>, and in the meantime, we might devise adequate safeguards. It’s even possible that the mass-disemployment effect of current, relatively dumb A.I. systems (e.g., Chat-GPT, Midjourney, Dall-E-2) will result in hard curbs on A.I. in most countries, by “popular demand.” <em>That</em> would mark a hard turn in our culture, though I wonder how long we could sustain it.</li>
</ol>
</li>
</ol>
<ol>
<li style="list-style-type: none;">
<ol start="4">
<li>Without a doubt, the media and entertainment industries are going to pick up on A.I. anxiety and start putting out more catastrophe/dystopia content in that genre. So even if we don’t <em>want</em> to think about all this, we’ll be more or less forced to do so.</li>
</ol>
</li>
</ol>
<p style="text-align: center;">***</p>
<p>&nbsp;</p>
<p>* First Law of Robotics: “A robot may not injure a human being or, through inaction, allow a human being to come to harm.”</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>THE LAST HISTORY AND THE END OF MAN</title>
		<link>/the-last-history-and-the-end-of-man/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Fri, 13 Jan 2023 04:45:23 +0000</pubDate>
				<category><![CDATA[cosmology]]></category>
		<category><![CDATA[human ecology]]></category>
		<category><![CDATA[ideas]]></category>
		<category><![CDATA[psychology]]></category>
		<category><![CDATA[science]]></category>
		<category><![CDATA[technology]]></category>
		<guid isPermaLink="false">/?p=762</guid>

					<description><![CDATA[Why most planetary civilizations collapse &#160; I didn’t get into video games until I was in my 40s. Oddly enough, it was a historian who triggered my interest. Niall Ferguson, the bestselling author, columnist, TV personality and Stanford professor, penned a 2006 New York Magazine piece, “How to Win a War,” that persuasively extolled the &#8230; <a href="/the-last-history-and-the-end-of-man/" class="more-link">Continue reading<span class="screen-reader-text"> "THE LAST HISTORY AND THE END OF MAN"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Why most planetary civilizations collapse</em></p>
<p><span id="more-762"></span></p>
<p>&nbsp;</p>
<p>I didn’t get into video games until I was in my 40s. Oddly enough, it was a historian who triggered my interest. Niall Ferguson, the bestselling author, columnist, TV personality and Stanford professor, penned a 2006 <em>New York Magazine</em> <a href="https://nymag.com/news/features/22787/">piece</a>, “How to Win a War,” that persuasively extolled the virtues of video games as tools for learning about history. He was particularly impressed by a certain turn-based PC strategy game that purported to model World War II—playing it, he said, had seriously challenged some of his own beliefs about the war.</p>
<p>I was not as impressed when I played that particular game, and later a more sophisticated competitor. The limitations of consumer-level computers and developer teams meant that these games simply couldn’t model the dynamics of the WW2-era world very well. However, even at that very modest level of simulation, the experience of replaying a historical period <em>again and again</em>, for dozens to hundreds of playthroughs, did prompt some thoughts about history in general.</p>
<p>One was simply that replaying a given stretch of history, which is to say, generating one variant history after another, has the effect of diminishing the significance of any of those variants. Naturally, in the highly abstracted milieu of a video game, one expects to be far less sensitive to details than one would be in real life. But I noticed that I became progressively desensitized to the details of the real-life WW2 as well: they seemed less interesting and meaningful.</p>
<p>To put it another way, my picture of this period of history was no longer formed from one clear image-capture, but from many—and in that multiple exposure, so to speak, most details were nonrecurring; they therefore tended to fade away as the number of exposures grew.</p>
<p>Would real-life history look different each time if we could re-run it from the same initial starting point? It absolutely would. Even one modern country is an enormously complex and nonlinear system—it will <em>always</em> vary significantly in how it runs from the same starting conditions, and the details of its course will be hard to predict very far in advance. (Think of how hard it is for us to foresee the course of a much simpler nonlinear system, the weather.)</p>
<p>Even so, we almost never think of history in this way. Experience encourages us instead to think of any historical episode as a singular phenomenon—one unique block of spacetime, never to be repeated—and that in turn leads us to frame any history as a sets of events linked by cause-effect relationships. Typically, we also try to draw big lessons from it all: the “lessons of history.” By contrast, when we have the ability to simulate replays of that block of spacetime again and again, seeing how things play out differently each time<em>, </em>it makes the inherently probabilistic nature of history stand out much more sharply. We are, in effect, forced to face a reality we normally wouldn’t acknowledge.</p>
<p>To illustrate again with an extreme example: Suppose one had a large bucket filled with a million marbles, each with its own identifying number, and suddenly dumped them onto some perfectly flat, expansive surface—and recorded precisely how they all bounced and rolled and reached some final arrangement. To the average person, that “history” of the marbles wouldn’t be particularly interesting, would it? The average person would understand intuitively that this marble-history was basically random, would look different in every re-run, and had nothing to teach, other than that marbles reliably obey known laws of mechanics. For that reason, writing a detailed History of the Marbles—or worse, having a dozen marble historians write their own competing tomes—would be absurd. Possibly such histories would be of interest <em>to marbles</em>, who might be curious about all the individual collisions that had brought them to their present positions. But to beings capable of a wider perspective, a history of the marbles would seem pointless—a measuring of statistical noise, as mathematicians would say.</p>
<p style="text-align: center;">*</p>
<p>Apropos of all that, at some point in my WW2-gaming sojourns I came up with a weird thought-experiment:</p>
<p style="padding-left: 40px;"><em>Suppose the virtual soldiers and citizens populating any given playthrough had human-like feelings, and regarded that playthrough—their playthrough—as the only one that had ever happened? What would these virtual people do if I, as the Player-God above them, suddenly revealed to them the true nature of their existence—in other words, revealed their “history” as but one chance-ridden playthrough among many?</em></p>
<p>They would <em>despair</em>, wouldn’t they? Not only at the revelation that their existence was a mere simulation, but also in the recognition that it was <em>merely one of many variant, stochastically determined existences—</em>one semi-random timeline among thousands, or really <em>billions</em> considering the wider universe of players with their separate copies of the game. They would see that, <em>even as a simulation, their existence was effectively meaningless</em> in the grand scheme of things.</p>
<p>Someday, computer games may be invented that not only simulate human events with a high degree of complexity, but also, via the right hardware, imbue their human-like characters with some degree of consciousness. Given the situation of these simulated humans, aware that they are trapped in worlds of no meaning or consequence, we as godlike players will feel sorry for them. However, the sufferings of our virtual creatures should be the least of our worries at that point—for by then we should have recognized that, as creatures of no consequence ourselves, we are in the same damned boat.</p>
<p style="text-align: center;">*</p>
<p>Can that be true? Is what you or I experience as “real life” merely one probabilistically determined playthrough among an infinitude of them?</p>
<p>The short answer is: very likely yes. And this is arguably the most important revelation—or, if you like, compelling theory—produced by science to date. Moreover, the idea I propose here is that any human civilization capable of grasping this true nature of our reality will eventually enter a state of deep and chronic despair, which perhaps can end only in human extinction.</p>
<p>This putative process of discovery and despair has an interesting, foreshadowing parallel in the most famous Western account of human origin, that of Adam and Eve in the Garden of Eden. For we are, with our science, compulsively eating a forbidden, toxic fruit (of the Tree of Knowledge) and are thereby, in effect, exiling ourselves from the lush, blissfully ignorant existence we briefly had.</p>
<p>And this may not be just a human affliction. It may be one that always strikes species once they reach a certain level of technical and scientific advancement. If so, then plausibly it has already extinguished most of the smart species across the universe, and has made the rest avoidant lest they transmit to us truths we cannot handle. This would explain the paradox—“Fermi’s Paradox”—that the universe probably has incubated trillions upon trillions of alien civilizations, yet the latter’s visits to us appear to have been relatively few and furtive.</p>
<p style="text-align: center;">*</p>
<p>Science, as we know it, is a very recent development. Broadly speaking, it is one of the fruits of the Neolithic Revolution, which began in the Eastern Mediterranean about 15,000 years ago, and by about 1000 A.D. had spread to almost every human society. This major shift in the human lifeway, from nomadism to farming and settlement-building, triggered a rapid, self-catalyzing increase in the scale and complexity of our societies, and the development of many new institutions. Science, however, was one of the slowest to emerge—and as an ongoing, global institution, dominant over magic and religion, has existed for only about a century and a half.</p>
<p>The progress of science has been bittersweet. On the one hand, it has led to better living standards through better knowledge and technology—e.g., better crop yields, better sanitation, better medicines, and a vastly better understanding and command of our environment. On the other hand, it has relentlessly belied man’s instinctive, high opinion of himself as a special creature of God, “made in His image.”</p>
<p>One of the earliest and most famous examples of this type of psychologically problematic scientific knowledge was the idea (introduced by Copernicus in 1543, and later refined and popularized by Kepler and Galileo), that our planet is not at the center of the universe. It took hundreds of years and considerable technical developments in astronomy for this painful truth that <em>the universe does not revolve around us</em> to be accepted. But in a sense, we are still struggling to cope with the implications. If we are not situated centrally in the universe, how could it have been made specifically for us, as our religions have led us to believe? A cosmology that placed us in one wispy spiral arm of one nondescript galaxy among <em>trillions</em> of galaxies might have been an important step forward for our science—but it was a giant leap downward for our self-image.</p>
<p>Then, of course, there was Darwin. Humans as mere animals, evolutionary cousins of apes? Impossible! The Church resisted that theory as it had resisted Galileo and Copernicus. But by Darwin’s time, science was much stronger, the Church much weaker, and within only a few decades, serious opposition to the theory of evolution by natural selection started to fade away.</p>
<p>It was also becoming clear, by then, that Earth couldn’t have been around for only a few thousand years, as accounts such as Genesis implied. Empowered by the discovery of radioactivity and radioactive decay, geologists by the mid-1920s understood that Earth was formed <em>billions</em> of years ago. This implied that we, <em>H. sapiens,</em> are merely an incidental and very recently developed addition to our planet’s fauna. In fact, many paleontologists now suspect that, had that asteroid not hit our planet about 65 million years ago, largely wiping out the then-dominant dinosaurs, tool-making primates like us might never have evolved.</p>
<p>Since the end of the 1900s, cosmologists generally have been in agreement that our observable universe has existed for roughly ten billion years before our solar system was even formed. That means that humans are almost certainly latecomers to the higher intelligence club—and may be as primitive and uncomprehending, in relation to truly advanced species, as ants or amoebas are to us.</p>
<p>All this points to the conclusion that a God of the Universe, if anything like Him exists, has no special interest in humans; and, moreover, that all human “meaning” and “significance” is strictly local—strictly confined to our tiny speck of reality.</p>
<p style="text-align: center;">*</p>
<p>Probably like most people who grew up in the latter half of the 20<sup>th</sup> century, I’ve tended to react to these scientific revelations by ignoring them. To the extent that I did think about them, in my younger years, I assumed with vague optimism that humans someday, through better technology, could spread from one star system to another, and so on until they establish their universality, perhaps ultimately melding with whatever force or entity made the universe. I think it’s fair to say that a lot of other people, including prominent advocates of space exploration, still think the same way.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-763" src="/wp-content/uploads/2023/01/spacefaring2.jpg" alt="" width="556" height="682" /></p>
<p>I now see such optimism as a form of denial—a denial that is going to be harder and harder to maintain, as time goes on and we humans are increasingly confronted with the nature of our reality.</p>
<p>How we understand that reality is something that I expect will undergo various elaborations in the coming decades. But it should already be apparent that the idea we could ever “conquer the universe,” or in any way escape the utter insignificance of our existence, is naïve.</p>
<p>The most obvious (though not even the worst) part of the problem is that the universe is just unmanageably vast: larger than we can ever observe, expanding faster than light, and very likely infinite—which would mean that the human realm or contribution, in relation to the whole, could never be more than infinitesimal. This idea that space is effectively infinite the physicist and cosmology popularizer Brian Greene has <a href="https://www.amazon.com/Hidden-Reality-Parallel-Universes-Cosmos/dp/0307278123">described</a> as “consistent with all observations and . . . part of the cosmological model favored by many physicists and astronomers.”</p>
<p>The human mind is not really adapted for contemplating infinities, but as Greene has pointed out, a truly infinite universe would contain, at any moment, infinite numbers of worlds identical to ours, some moving through time precisely as ours does, others with variations—in fact, all possible variations.</p>
<p>Again, compared to the whole of this Infinite Universe, and, we might also say, in the eyes of its Creator, the histories of individual worlds within it, along with their systems of morality and meaning, should be of infinitesimal significance. If we could take a God’s-eye view, zooming out from our planet to encompass our whole galaxy, and then galaxy clusters, and clusters of clusters, we would see the histories of individual worlds much as the video game player sees our world: less as sets of interlinked events, and more as manifestations of a broader, stochastic process, whose function is essentially only <em>to ink over the space of possibility</em>.</p>
<p>Contemporary physics, specifically quantum mechanics, delivers us to an even colder, darker destination. Quantum mechanics has at its core an equation, the Schrödinger wave equation, that implies a weird multiplicity of states for any given quantum-scale particle (an electron, for example) traveling through time. Physicists in the early years of quantum theory clung to the belief that these multiple states somehow probabilistically “collapse” to one state whenever one tries to observe the particle with a measuring device. However, in the past few decades the field basically has abandoned that rather hand-waving interpretation, mostly in favor of a simpler, more parsimonious one: that the multiple possible states a particle can be observed to have are all, in a sense, <em>real</em>.</p>
<p>In other words, these alternate states represent multiple actual particles existing in different “worlds” or “universes.” Thus, a physicist recording the impact of one particular state of an electron has, at that moment, otherwise identical counterparts in otherwise identical alternate universes who record the impacts of all the other states.</p>
<p>The reality implied by this interpretation—now called the Many Worlds Interpretation (MWI)—encompasses not just one very big universe but, rather, an infinite number of them, a “multiverse,” across which everything that can happen does happen. There is a perfection here that, at least in a technical sense, should impress those who always believed Creation would be flawless and complete.</p>
<p>Of course, from the usual sentimental human perspective, MWI looks bizarre and horrifying. Even so, its superior simplicity and parsimony, as a way of thinking about quantum phenomena, has enabled it to survive and spread despite its implications—which physicists don’t “like” any more than you or I do.</p>
<p>As Greene has <a href="https://www.amazon.com/Hidden-Reality-Parallel-Universes-Cosmos/dp/0307278123">noted</a>:</p>
<p style="padding-left: 40px;">I find it both curious and compelling that numerous developments in physics, if followed sufficiently far, bump into some variation on the parallel-universe theme.</p>
<p style="text-align: center;">*</p>
<p>“The multiverse will drive you crazy if you really think about how it affects your life, and I can’t live like that,” the philosopher of physics and MWI theorist Simon Saunders once told a <a href="https://www.newscientist.com/article/mg19526223-700-parallel-universes-make-quantum-sense/">reporter</a>. “I’ll just accept [it] and then think about something else, to save my sanity.”</p>
<p>Is <em>thinking about something else </em>a viable strategy to escape the psychological consequences of modern cosmology?</p>
<p>Conceivably it is, up to a point. Humans evolved with basic, powerful drives towards survival and procreation, and even religiosity; they thus probably have, on average, a significant innate resistance to nihilist worldviews. Even now, well into the third millennium A.D., most of the human population professes belief in one religion or another. Also, obviously, the average person has no deep understanding of, or interest in, MWI or other modern cosmological theories.</p>
<p>Yet the things we do learn and think about ultimately affect our behavior, if only subconsciously. One doesn’t have to be a philosopher or a psychologist to understand—to take another extreme example—that if we all knew our solar system would be obliterated within a year, making it obvious that our existence was and always had been inconsequential, enough of us would fall into despair that our societies would start to disintegrate immediately.</p>
<p>I think the reason we’ve largely been able, so far, to resist the toxic implications of modern cosmology is simply that we haven’t been forced to confront them. But that situation is changing.</p>
<p>When I was growing up in the 1970s and early 80s, cosmology was expansive but still quite tame compared to what was coming. Carl Sagan’s 1980 <em>Cosmos</em> TV series on PBS, for example, was hardly despair-inducing. One could contemplate the large universe depicted by Sagan and other pop cosmologists of the time, and, as I noted above, could still fantasize about humans’ someday traversing and conquering it. MWI and other infinite-universe theories had not yet caught on, certainly not at the popular level.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-764" src="/wp-content/uploads/2023/01/sagan-cosmos.jpg" alt="" width="714" height="440" /></p>
<p>These days, by contrast, MWI and similar “parallel universe” themes are essential elements of pop cosmology, and, perhaps more importantly, are also common in <a href="https://en.wikipedia.org/wiki/Everything_Everywhere_All_at_Once">pop culture</a> generally.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-765" src="/wp-content/uploads/2023/01/everything-ev.jpg" alt="" width="800" height="399" srcset="/wp-content/uploads/2023/01/everything-ev.jpg 800w, /wp-content/uploads/2023/01/everything-ev-768x383.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" /></p>
<p>Moreover, although technologies based on quantum mechanics (such as lasers) have been around for decades, newer quantum tech such as quantum computing and quantum encryption emphasizes, for the first time, the spookier, multiplicity-of-states aspect of quantum mechanics—the aspect that MWI essentially was devised to explain. Thus, from popular science to tech to popular media culture generally, people are being exposed to the infinite-universe/multiverse idea as never before, and in ever-stronger doses.</p>
<p>The impact of that rising exposure won’t be immediately obvious. There are, and in the coming decades will continue to be, many other drivers of despair, disruption, suicide, and social disintegration in the modern world—drivers such as <a href="https://thoughtsofstone.github.io/cultural-feminization-an-introduction/">cultural feminization</a>, mass immigration, and <a href="https://thoughtsofstone.github.io/the-ouroboros-economy/">human-displacing AI systems</a>. Trying to disentangle the effect of one of these from the others is going to be challenging, to put it mildly. But, if my hypothesis is correct, “cosmological despair” will weigh more and more heavily and evidently on developed societies—especially among younger people, who will encounter MWI and similarly harsh cosmologies in their formative years, never having had the comforts of older, friendlier worldviews. In other words, if the world is now entering an Age of Despair principally for other reasons, cosmology will keep it there terminally.</p>
<p style="text-align: center;">*</p>
<p>There probably aren’t very many clear examples, yet, of people taking their own lives as a result of belief in MWI or other toxic cosmologies. However, something like this seems to have happened in the case of Hugh Everett III—the physicist who developed the original version of MWI (“<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwieh-2-sbb8AhXvNlkFHcDLBqwQFnoECAkQAQ&amp;url=http%3A%2F%2Fwww.weylmann.com%2Frelative_state.pdf">’Relative State’ Formulation of Quantum Mechanics</a>”) as his Princeton PhD thesis in 1956.</p>
<p>Everett eventually became a financially successful tech entrepreneur and, in most ways seemed normal, being married with children, having friends, and pursuing ordinary hobbies and pleasures that included wine-making and ocean liner cruises. However . . .</p>
<p style="padding-left: 40px;">Everett firmly believed that his many-worlds theory guaranteed him immortality: His consciousness, he argued, is bound at each branching to follow whatever path does not lead to death—and so on ad infinitum. [<a href="https://space.mit.edu/home/tegmark/everett/everett.html#e24">link</a>]</p>
<p>Probably at least partly due to this belief, he smoked, drank, and ate with abandon, which ultimately gave him a <a href="https://www.washingtonpost.com/archive/local/1982/07/23/dr-hugh-everett-iii-founder-of-data-firm/16fc45d5-0e5e-445e-9714-12550bb6354e/">fatal heart attack</a> in 1982, when he was only 51 years old. In accordance with his wishes, his body was cremated and his ashes were thrown out with other household garbage.</p>
<p>A decade and a half later, Everett’s troubled 39-year-old daughter Liz took her own life even more directly. She left a note to the effect that she wanted her own ashes thrown out with the garbage, so that she might “end up in the correct parallel universe to meet up w[ith] Daddy.”</p>
<p>&nbsp;</p>
<p style="text-align: center;">***</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>THE OUROBOROS ECONOMY</title>
		<link>/the-ouroboros-economy/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Fri, 02 Dec 2022 04:32:55 +0000</pubDate>
				<category><![CDATA[economics]]></category>
		<category><![CDATA[fall of the West]]></category>
		<category><![CDATA[human ecology]]></category>
		<category><![CDATA[immigration]]></category>
		<category><![CDATA[psychology]]></category>
		<category><![CDATA[technology]]></category>
		<category><![CDATA[USA]]></category>
		<guid isPermaLink="false">/?p=730</guid>

					<description><![CDATA[You are about to become obsolete. &#160; When labor becomes scarce, expensive, and/or unreliable, business owners start looking for alternatives. For most of the past 30 years, a very attractive alternative was offshoring—to countries like China, where labor was cheap, plentiful, and reliable. In the past three years, the COVID pandemic and the maturing of &#8230; <a href="/the-ouroboros-economy/" class="more-link">Continue reading<span class="screen-reader-text"> "THE OUROBOROS ECONOMY"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>You are about to become obsolete.<br />
</em></p>
<p><span id="more-730"></span></p>
<p>&nbsp;</p>
<p>When labor becomes scarce, expensive, and/or unreliable, business owners start looking for alternatives. For most of the past 30 years, a very attractive alternative was offshoring—to countries like China, where labor was cheap, plentiful, and reliable. In the past three years, the COVID pandemic and the maturing of once-cheap labor markets, plus the increasing obviousness of China’s IP theft and overall hegemonic ambitions, have begun to reverse that trend. Economists are now forecasting “the end of globalization,” with labor scarcity <a href="https://www.conference-board.org/topics/recession/how-high-will-US-unemployment-go">continuing</a> for decades as the workforce shrinks. Big companies, desperate for workers, are even indicating a willingness to hire people <a href="https://www.wsj.com/articles/employers-rethink-need-for-college-degrees-in-tight-labor-market-11669432133">without college degrees </a>for positions that traditionally required them.</p>
<p>To me, though, the idea that labor will continue to be scarce seems wrong. As I see it, mechanization and AI are now moving onto the steepest part of the innovation slope, and will soon start “disemploying” people all the way up the labor value chain, from manual trades to those overpaid millennial marketing girls sipping lattes on TikTok. Even I, with my fairly challenging profession and decades of experience, am likely to be left jobless at least a few years before I’d like to retire.</p>
<p>AI has taken longer than expected to arrive in useful forms, but is now definitely arriving and ready to start disrupting. It can, technically if not yet legally, drive cars, tractors, trains, and boats; fly planes and drones; and guard warehouses. The mechanization technology underlying humanoid robots has been making big advances too—such robots now can open doors, climb stairs, recover from falls, hold and manipulate heavy objects, etc. Once such robots are mass-produced and made available for leasing, their use as replacements for factory workers, waiters, construction workers, checkout clerks, etc. will become a viable proposition. Will we have to wait as long as five years before that starts?</p>
<p>AI language-processing software that can be taught, or can teach itself via the Internet, should start displacing office worker bees well before then—and by worker bees I mean basically anyone whose job consists largely of emailing, writing reports, filling out spreadsheets, and doing other routine kinds of paperwork. And we’ve all seen the AI text-to-image and text-to-movie <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">packages</a> that were unleashed recently and have been improving at a rapid pace. How long will it be before a single writer, working with one of those algos, generates a feature-length film on his own? A year from now? Two?</p>
<p>Essentially, we’re facing the prospect of the abrupt end of the labor market, an institution that has been at the center of human civilization for millennia.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-732" src="/wp-content/uploads/2022/12/silent-running.jpg" alt="" width="1200" height="638" srcset="/wp-content/uploads/2022/12/silent-running.jpg 1200w, /wp-content/uploads/2022/12/silent-running-768x408.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p>I’m aware that critics of earlier forms of labor-saving technology, such as the Luddites and their ilk, were somewhat shortsighted in their predictions of mass labor displacement. These early industrial workers were themselves displaced in large numbers, and in that sense had every reason to complain. What they failed to see was that the labor-saving innovations that displaced them would, on the whole, lead to greater productivity and economic growth, and ultimately a net rise in demand for labor.</p>
<p>But that was then, and this is now. The tech that’s going to be released into the world in this decade will be capable of displacing humans from their jobs much faster than the latter will be able to keep up. In other words, if you are laid off because your employer or clients can just buy an AI package to do the same job more cheaply, and you then decide to retrain for some “AI-proof” job, it’s quite likely that that “AI-proof” job will be overtaken by AI long before you can get into it. Even if that job stays available, you’d be competing for it with an exponentially rising number of other displaced human workers.</p>
<p>It’s impossible to predict in detail how this will all play out. But I can easily imagine an early phase in which language-processing AI, vehicle AI, warehouse robots, and a few other related innovations are hailed as game-changers for businesses and other organizations, allowing them to do much more with fewer workers and at less cost—and alleviating inflationary labor shortages along the way. Close on the heels of that “denial” phase, though, will come the bargaining, depression, and acceptance phases, as the pace of disemployment accelerates. I see this as an ouroboros—snake-eating-its-tail—process, because it involves the economy effectively consuming itself, i.e., destroying, with every increment of growth and investment in innovation, the employment earnings that are the principal fuel for a modern economy.</p>
<p>There may be no stable equilibrium in this process for a long while. Governments probably will try to tax businesses, especially AI-using businesses, to fund welfare payments to the unemployed masses, but will that work? Even if governments could manage it fiscally, what would be the psychological effect on tens of millions of people who can no longer earn a living for themselves? (We already know that <a href="https://blogs.lse.ac.uk/politicsandpolicy/men-are-more-likely-to-suffer-adverse-health-consequences-as-a-result-of-unemployment-than-women/">men become easily depressed when unemployed</a>.)</p>
<p><img decoding="async" class="aligncenter size-full wp-image-734" src="/wp-content/uploads/2022/12/wall-e.jpg" alt="" width="790" height="331" srcset="/wp-content/uploads/2022/12/wall-e.jpg 790w, /wp-content/uploads/2022/12/wall-e-768x322.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" /></p>
<p>It also seems unlikely that Western governments would ever simply disallow the use of AI and robotics. One of the great lessons of the mass immigration era is that Western governments, ostensibly “democratic,” <em>like</em> having electorates made up of financially stressed people whose votes can be bought with government largesse.</p>
<p>It stands to reason that the disemployment situation will be easier in countries that currently have relatively small workforces—or rely on guest workers who can be sent quickly back to their home countries if needed. By the same logic, countries with open borders and huge, low-skill, permanent immigrant populations, like the US, could be in serious trouble. Those countries will suddenly have many millions of excess mouths to feed, and to do so might easily require taxation levels that trigger capital flight.</p>
<p>I’m not totally averse to the idea that at the end of this transition lies a society in which robots do everything for near-zero cost and humans can stay busy however they like without worrying much about money. But it’s hard to believe this transition will occur without historic levels of pain. I’ve written often in this space about various drivers of Western decline, collapse, and general upheaval; the now-imminent “ouroboros economy” of AI and robotics is surely another one.</p>
<p style="text-align: center;">***</p>
<p>&nbsp;</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>THE TREE OF KNOWLEDGE</title>
		<link>/the-tree-of-knowledge/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Sat, 16 Apr 2022 21:52:30 +0000</pubDate>
				<category><![CDATA[ideas]]></category>
		<category><![CDATA[psychology]]></category>
		<category><![CDATA[technology]]></category>
		<guid isPermaLink="false">/?p=566</guid>

					<description><![CDATA[Encourage new ideas and more productive dialogue by giving idea-originators the credit they’re due &#160; Late in 2011, I first put pen to paper, or fingertips to keyboard, to describe an idea that had been bouncing around in my mind for several years—at least since Larry Summers’s public defenestration from Harvard in 2005. The idea &#8230; <a href="/the-tree-of-knowledge/" class="more-link">Continue reading<span class="screen-reader-text"> "THE TREE OF KNOWLEDGE"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Encourage new ideas and more productive dialogue by giving idea-originators the credit they’re due</em></p>
<p><span id="more-566"></span></p>
<p>&nbsp;</p>
<p>Late in 2011, I first put pen to paper, or fingertips to keyboard, to describe an idea that had been bouncing around in my mind for several years—at least since Larry Summers’s public <a href="https://thoughtsofstone.github.io/the-day-the-logic-died/">defenestration</a> from Harvard in 2005. The idea was that women’s unprecedented mass entry into public life, in the US and other Western societies, had been “feminizing” public discourse and policy—leading among other things to the rise of political correctness culture, the increasing suppression of free speech, and the decline of capital punishment—due to women’s different way of thinking about the world.</p>
<p>I wasn’t a professional opinionator—just an amateur with a background in journalism. But I did have a website where I published occasional essays, which were read by a few hundreds to thousands of people every month, and there I first set down my thoughts in a <a href="https://james-the-obscure.github.io/the-demise-of-guythink/">short piece</a> about the demise of the male cognitive style. In the ensuing decade, I developed the basic idea in further essays, including in two web magazines with significant readership. I joined Twitter to promote these essays. Eventually I had some recognition as an introducer of this idea.</p>
<p><img decoding="async" class="size-full wp-image-573 aligncenter" src="/wp-content/uploads/2022/04/spotted-toad.jpg" alt="" width="591" height="224" srcset="/wp-content/uploads/2022/04/spotted-toad.jpg 591w, /wp-content/uploads/2022/04/spotted-toad-300x114.jpg 300w" sizes="(max-width: 591px) 85vw, 591px" /></p>
<p><img decoding="async" class="wp-image-574 aligncenter" src="/wp-content/uploads/2022/04/cowen-Copy.jpg" alt="" width="471" height="310" srcset="/wp-content/uploads/2022/04/cowen-Copy.jpg 660w, /wp-content/uploads/2022/04/cowen-Copy-300x198.jpg 300w" sizes="(max-width: 471px) 85vw, 471px" /></p>
<p><img decoding="async" class="wp-image-575 aligncenter" src="/wp-content/uploads/2022/04/pink-shift-cover.jpg" alt="" width="364" height="527" srcset="/wp-content/uploads/2022/04/pink-shift-cover.jpg 1246w, /wp-content/uploads/2022/04/pink-shift-cover-207x300.jpg 207w, /wp-content/uploads/2022/04/pink-shift-cover-707x1024.jpg 707w, /wp-content/uploads/2022/04/pink-shift-cover-768x1113.jpg 768w, /wp-content/uploads/2022/04/pink-shift-cover-1060x1536.jpg 1060w, /wp-content/uploads/2022/04/pink-shift-cover-1200x1738.jpg 1200w" sizes="(max-width: 364px) 85vw, 364px" /></p>
<p>The idea of cultural feminization became more and more obvious in the wake of the Great Awokening, which appeared to be a heavily female-dominated social phenomenon. One <a href="https://thecritic.co.uk/issues/december-january-2022/new-female-ascendency/">writer</a> after <a href="https://www.nytimes.com/2022/01/12/opinion/gender-gap-politics.html">another</a> began to notice and embrace the cultural feminization idea as their own—either not knowing of my (and others’) prior <a href="https://thoughtsofstone.github.io/cultural-feminization-a-bibliography/">contributions</a>, or knowing of them but not considering them “big” enough, in terms of readership and public awareness, to acknowledge them.</p>
<p>I may be ultra-sensitive on this topic, because some of my past journalistic work has been used by others with inadequate attribution (or none). I am also aware that this is a very personal kind of pain, one that tends to attract little sympathy from those who have not had a similar experience.</p>
<div style="width: 640px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-566-1" width="640" height="352" preload="metadata" controls="controls"><source type="video/mp4" src="/wp-content/uploads/2022/04/Dustin-Hoffman-And-Robert-De-Niro-Wag-The-Dog-I-Want-The-Credit-2-1-Copy.mp4?_=1" /><a href="/wp-content/uploads/2022/04/Dustin-Hoffman-And-Robert-De-Niro-Wag-The-Dog-I-Want-The-Credit-2-1-Copy.mp4">/wp-content/uploads/2022/04/Dustin-Hoffman-And-Robert-De-Niro-Wag-The-Dog-I-Want-The-Credit-2-1-Copy.mp4</a></video></div>
<p>Still, I think there is an objective case to be made, wholly apart from my own gripes, that “inadequate credit to originators of ideas” (ICOI?) is a legitimate social problem whose solution would materially benefit society. Why? Simply because denying people credit for developing original ideas effectively disincentivizes them from making the effort to develop those ideas, and in that straightforward sense is likely to retard human progress. Obscuring past expressions of an idea also limits new discussions of that idea when they do occur—people are constantly “reinventing the wheel.”</p>
<p>We acknowledge this logic in other domains of knowledge production. The new ideas of scientists, doctors, English Lit experts, etc. are protected and made accessible by the searchability of their formal papers (via the PubMed system, for example) and the citation custom. New technical ideas also are protected and made accessible by the patent system and its strong judicial backing. Books, musical compositions, movies, TV shows, are protected—albeit weakly—by the copyright system.</p>
<p>Unfortunately, when it comes to ordinary, non-patentable, non-copyrightable ideas that are published in informal, non-academic fora—ideas such as “women’s mass entry into public life has feminized institutions, culture and policy”—there is no legal protection, and no good and searchable repository. Moreover, the principal search engines for web-based material aren’t really designed to uncover the origins and subsequent iterations of ideas. Mainly because of this, there is only a very weak cultural enforcement of priority claims by idea-originators. I observe anecdotally, for example, that Twitter is full of DIY experts who (from lack of knowledge and/or lack of scruples) recycle others’ ideas, framing their contributions as new, though they are unoriginal and are often incomplete/undeveloped compared to the true original. Thus, the true originators are discouraged by not getting credit, and the false ones lead everyone in circles instead of moving the discussion productively forward.</p>
<p><strong>Solutions?</strong></p>
<p>In a broad-brush way, I imagine two complementary solutions to this problem. One is to develop a “registry of ideas.” The other is to develop an “idea history” search engine/bot.</p>
<p>The registry of ideas could have a broad structure like the “<a href="https://en.wikipedia.org/wiki/Motif-Index_of_Folk-Literature">motif index</a>” developed by the anthropologist Stith Thompson in the 1930s to record and categorize folkloric tales according to their different elements or &#8220;motifs.”</p>
<p><img decoding="async" class="wp-image-569 aligncenter" src="/wp-content/uploads/2022/04/motif-index-Copy-2-scaled.jpg" alt="" width="455" height="736" srcset="/wp-content/uploads/2022/04/motif-index-Copy-2-scaled.jpg 1584w, /wp-content/uploads/2022/04/motif-index-Copy-2-186x300.jpg 186w, /wp-content/uploads/2022/04/motif-index-Copy-2-633x1024.jpg 633w, /wp-content/uploads/2022/04/motif-index-Copy-2-768x1241.jpg 768w, /wp-content/uploads/2022/04/motif-index-Copy-2-950x1536.jpg 950w, /wp-content/uploads/2022/04/motif-index-Copy-2-1267x2048.jpg 1267w" sizes="(max-width: 455px) 85vw, 455px" /></p>
<p>The Dewey Decimal System, by which librarians decide where to shelve books, and the aborted Google “<a href="https://dbpedia.org/page/Knol">Knol</a>” project, offer other potentially helpful models of topic-categorization.</p>
<p>I imagine that some organization like the Wikimedia Foundation (Wikipedia’s parent) could provide some initial structure and guidelines for such a database, after which it would be assembled and curated by volunteers in a highly distributed manner—like Wikipedia. Idea-originators could directly submit their work for recording in the registry, as they can now for appropriate intellectual property with the patent and copyright systems, or they could just let the curators detect their work (assuming it had been published) with their specially designed web bots and other search tools.</p>
<p>Those tools would be powered by “idea history” search algos, which would parse natural language text to identify ideas being expressed, and categorize them—at least as a “first pass” effort to be checked by human curators of different idea domains, though AI tech may soon be able to handle such tasks on its own.</p>
<p>I suppose one objection to this whole idea—this idea about ideas—would be that ordinary writers, especially casual self-published essayists and social-media posters, won’t submit to a citation system the way academics do. I think, though, that to a great extent, ordinary writers already <em>do</em> follow the custom of citation. These days (unlike, say, 50 years ago) a large majority of young adults in the US have had some college experience, with the exposure to citation rules that entails. More importantly, the internet with its “hypertext” ability that can easily link text to other documents anywhere on the web has made citation of others’ work routine even for the most casual writers. The only things missing, really, are the search engines and repositories that would help to fully routinize the assignment of priority, rewarding creativity appropriately, and plausibly making even our humdrum, everyday dialogue a bit more rigorous and serious.</p>
<p style="text-align: center;">* * *</p>
]]></content:encoded>
					
		
		<enclosure url="/wp-content/uploads/2022/04/Dustin-Hoffman-And-Robert-De-Niro-Wag-The-Dog-I-Want-The-Credit-2-1-Copy.mp4" length="1281012" type="video/mp4" />

			</item>
	</channel>
</rss>
