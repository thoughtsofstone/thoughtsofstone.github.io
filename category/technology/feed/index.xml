<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>technology &#8211; Thoughts of Stone</title>
	<atom:link href="https://thoughtsofstone.github.io/category/technology/feed/" rel="self" type="application/rss+xml" />
	<link>https://thoughtsofstone.github.io/</link>
	<description>short essays, usually about humans</description>
	<lastBuildDate>Fri, 13 Jan 2023 04:56:27 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.1.1</generator>

<image>
	<url>https://thoughtsofstone.github.io/wp-content/uploads/2020/07/cropped-icon-32x32.jpg</url>
	<title>technology &#8211; Thoughts of Stone</title>
	<link>https://thoughtsofstone.github.io/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>THE LAST HISTORY AND THE END OF MAN</title>
		<link>https://thoughtsofstone.github.io/the-last-history-and-the-end-of-man/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Fri, 13 Jan 2023 04:45:23 +0000</pubDate>
				<category><![CDATA[cosmology]]></category>
		<category><![CDATA[human ecology]]></category>
		<category><![CDATA[ideas]]></category>
		<category><![CDATA[psychology]]></category>
		<category><![CDATA[science]]></category>
		<category><![CDATA[technology]]></category>
		<guid isPermaLink="false">https://thoughtsofstone.github.io/?p=762</guid>

					<description><![CDATA[Why most planetary civilizations collapse &#160; I didn’t get into video games until I was in my 40s. Oddly enough, it was a historian who triggered my interest. Niall Ferguson, the bestselling author, columnist, TV personality and Stanford professor, penned a 2006 New York Magazine piece, “How to Win a War,” that persuasively extolled the &#8230; <a href="https://thoughtsofstone.github.io/the-last-history-and-the-end-of-man/" class="more-link">Continue reading<span class="screen-reader-text"> "THE LAST HISTORY AND THE END OF MAN"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Why most planetary civilizations collapse</em></p>
<p><span id="more-762"></span></p>
<p>&nbsp;</p>
<p>I didn’t get into video games until I was in my 40s. Oddly enough, it was a historian who triggered my interest. Niall Ferguson, the bestselling author, columnist, TV personality and Stanford professor, penned a 2006 <em>New York Magazine</em> <a href="https://nymag.com/news/features/22787/">piece</a>, “How to Win a War,” that persuasively extolled the virtues of video games as tools for learning about history. He was particularly impressed by a certain turn-based PC strategy game that purported to model World War II—playing it, he said, had seriously challenged some of his own beliefs about the war.</p>
<p>I was not as impressed when I played that particular game, and later a more sophisticated competitor. The limitations of consumer-level computers and developer teams meant that these games simply couldn’t model the dynamics of the WW2-era world very well. However, even at that very modest level of simulation, the experience of replaying a historical period <em>again and again</em>, for dozens to hundreds of playthroughs, did prompt some thoughts about history in general.</p>
<p>One was simply that replaying a given stretch of history, which is to say, generating one variant history after another, has the effect of diminishing the significance of any of those variants. Naturally, in the highly abstracted milieu of a video game, one expects to be far less sensitive to details than one would be in real life. But I noticed that I became progressively desensitized to the details of the real-life WW2 as well: they seemed less interesting and meaningful.</p>
<p>To put it another way, my picture of this period of history was no longer formed from one clear image-capture, but from many—and in that multiple exposure, so to speak, most details were nonrecurring; they therefore tended to fade away as the number of exposures grew.</p>
<p>Would real-life history look different each time if we could re-run it from the same initial starting point? It absolutely would. Even one modern country is an enormously complex and nonlinear system—it will <em>always</em> vary significantly in how it runs from the same starting conditions, and the details of its course will be hard to predict very far in advance. (Think of how hard it is for us to foresee the course of a much simpler nonlinear system, the weather.)</p>
<p>Even so, we almost never think of history in this way. Experience encourages us instead to think of any historical episode as a singular phenomenon—one unique block of spacetime, never to be repeated—and that in turn leads us to frame any history as a sets of events linked by cause-effect relationships. Typically, we also try to draw big lessons from it all: the “lessons of history.” By contrast, when we have the ability to simulate replays of that block of spacetime again and again, seeing how things play out differently each time<em>, </em>it makes the inherently probabilistic nature of history stand out much more sharply. We are, in effect, forced to face a reality we normally wouldn’t acknowledge.</p>
<p>To illustrate again with an extreme example: Suppose one had a large bucket filled with a million marbles, each with its own identifying number, and suddenly dumped them onto some perfectly flat, expansive surface—and recorded precisely how they all bounced and rolled and reached some final arrangement. To the average person, that “history” of the marbles wouldn’t be particularly interesting, would it? The average person would understand intuitively that this marble-history was basically random, would look different in every re-run, and had nothing to teach, other than that marbles reliably obey known laws of mechanics. For that reason, writing a detailed History of the Marbles—or worse, having a dozen marble historians write their own competing tomes—would be absurd. Possibly such histories would be of interest <em>to marbles</em>, who might be curious about all the individual collisions that had brought them to their present positions. But to beings capable of a wider perspective, a history of the marbles would seem pointless—a measuring of statistical noise, as mathematicians would say.</p>
<p style="text-align: center;">*</p>
<p>Apropos of all that, at some point in my WW2-gaming sojourns I came up with a weird thought-experiment:</p>
<p style="padding-left: 40px;"><em>Suppose the virtual soldiers and citizens populating any given playthrough had human-like feelings, and regarded that playthrough—their playthrough—as the only one that had ever happened? What would these virtual people do if I, as the Player-God above them, suddenly revealed to them the true nature of their existence—in other words, revealed their “history” as but one chance-ridden playthrough among many?</em></p>
<p>They would <em>despair</em>, wouldn’t they? Not only at the revelation that their existence was a mere simulation, but also in the recognition that it was <em>merely one of many variant, stochastically determined existences—</em>one semi-random timeline among thousands, or really <em>billions</em> considering the wider universe of players with their separate copies of the game. They would see that, <em>even as a simulation, their existence was effectively meaningless</em> in the grand scheme of things.</p>
<p>Someday, computer games may be invented that not only simulate human events with a high degree of complexity, but also, via the right hardware, imbue their human-like characters with some degree of consciousness. Given the situation of these simulated humans, aware that they are trapped in worlds of no meaning or consequence, we as godlike players will feel sorry for them. However, the sufferings of our virtual creatures should be the least of our worries at that point—for by then we should have recognized that, as creatures of no consequence ourselves, we are in the same damned boat.</p>
<p style="text-align: center;">*</p>
<p>Can that be true? Is what you or I experience as “real life” merely one probabilistically determined playthrough among an infinitude of them?</p>
<p>The short answer is: very likely yes. And this is arguably the most important revelation—or, if you like, compelling theory—produced by science to date. Moreover, the idea I propose here is that any human civilization capable of grasping this true nature of our reality will eventually enter a state of deep and chronic despair, which perhaps can end only in human extinction.</p>
<p>This putative process of discovery and despair has an interesting, foreshadowing parallel in the most famous Western account of human origin, that of Adam and Eve in the Garden of Eden. For we are, with our science, compulsively eating a forbidden, toxic fruit (of the Tree of Knowledge) and are thereby, in effect, exiling ourselves from the lush, blissfully ignorant existence we briefly had.</p>
<p>And this may not be just a human affliction. It may be one that always strikes species once they reach a certain level of technical and scientific advancement. If so, then plausibly it has already extinguished most of the smart species across the universe, and has made the rest avoidant lest they transmit to us truths we cannot handle. This would explain the paradox—“Fermi’s Paradox”—that the universe probably has incubated trillions upon trillions of alien civilizations, yet the latter’s visits to us appear to have been relatively few and furtive.</p>
<p style="text-align: center;">*</p>
<p>Science, as we know it, is a very recent development. Broadly speaking, it is one of the fruits of the Neolithic Revolution, which began in the Eastern Mediterranean about 15,000 years ago, and by about 1000 A.D. had spread to almost every human society. This major shift in the human lifeway, from nomadism to farming and settlement-building, triggered a rapid, self-catalyzing increase in the scale and complexity of our societies, and the development of many new institutions. Science, however, was one of the slowest to emerge—and as an ongoing, global institution, dominant over magic and religion, has existed for only about a century and a half.</p>
<p>The progress of science has been bittersweet. On the one hand, it has led to better living standards through better knowledge and technology—e.g., better crop yields, better sanitation, better medicines, and a vastly better understanding and command of our environment. On the other hand, it has relentlessly belied man’s instinctive, high opinion of himself as a special creature of God, “made in His image.”</p>
<p>One of the earliest and most famous examples of this type of psychologically problematic scientific knowledge was the idea (introduced by Copernicus in 1543, and later refined and popularized by Kepler and Galileo), that our planet is not at the center of the universe. It took hundreds of years and considerable technical developments in astronomy for this painful truth that <em>the universe does not revolve around us</em> to be accepted. But in a sense, we are still struggling to cope with the implications. If we are not situated centrally in the universe, how could it have been made specifically for us, as our religions have led us to believe? A cosmology that placed us in one wispy spiral arm of one nondescript galaxy among <em>trillions</em> of galaxies might have been an important step forward for our science—but it was a giant leap downward for our self-image.</p>
<p>Then, of course, there was Darwin. Humans as mere animals, evolutionary cousins of apes? Impossible! The Church resisted that theory as it had resisted Galileo and Copernicus. But by Darwin’s time, science was much stronger, the Church much weaker, and within only a few decades, serious opposition to the theory of evolution by natural selection started to fade away.</p>
<p>It was also becoming clear, by then, that Earth couldn’t have been around for only a few thousand years, as accounts such as Genesis implied. Empowered by the discovery of radioactivity and radioactive decay, geologists by the mid-1920s understood that Earth was formed <em>billions</em> of years ago. This implied that we, <em>H. sapiens,</em> are merely an incidental and very recently developed addition to our planet’s fauna. In fact, many paleontologists now suspect that, had that asteroid not hit our planet about 65 million years ago, largely wiping out the then-dominant dinosaurs, tool-making primates like us might never have evolved.</p>
<p>Since the end of the 1900s, cosmologists generally have been in agreement that our observable universe has existed for roughly ten billion years before our solar system was even formed. That means that humans are almost certainly latecomers to the higher intelligence club—and may be as primitive and uncomprehending, in relation to truly advanced species, as ants or amoebas are to us.</p>
<p>All this points to the conclusion that a God of the Universe, if anything like Him exists, has no special interest in humans; and, moreover, that all human “meaning” and “significance” is strictly local—strictly confined to our tiny speck of reality.</p>
<p style="text-align: center;">*</p>
<p>Probably like most people who grew up in the latter half of the 20<sup>th</sup> century, I’ve tended to react to these scientific revelations by ignoring them. To the extent that I did think about them, in my younger years, I assumed with vague optimism that humans someday, through better technology, could spread from one star system to another, and so on until they establish their universality, perhaps ultimately melding with whatever force or entity made the universe. I think it’s fair to say that a lot of other people, including prominent advocates of space exploration, still think the same way.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-763" src="https://thoughtsofstone.github.io/wp-content/uploads/2023/01/spacefaring2.jpg" alt="" width="556" height="682" /></p>
<p>I now see such optimism as a form of denial—a denial that is going to be harder and harder to maintain, as time goes on and we humans are increasingly confronted with the nature of our reality.</p>
<p>How we understand that reality is something that I expect will undergo various elaborations in the coming decades. But it should already be apparent that the idea we could ever “conquer the universe,” or in any way escape the utter insignificance of our existence, is naïve.</p>
<p>The most obvious (though not even the worst) part of the problem is that the universe is just unmanageably vast: larger than we can ever observe, expanding faster than light, and very likely infinite—which would mean that the human realm or contribution, in relation to the whole, could never be more than infinitesimal. This idea that space is effectively infinite the physicist and cosmology popularizer Brian Greene has <a href="https://www.amazon.com/Hidden-Reality-Parallel-Universes-Cosmos/dp/0307278123">described</a> as “consistent with all observations and . . . part of the cosmological model favored by many physicists and astronomers.”</p>
<p>The human mind is not really adapted for contemplating infinities, but as Greene has pointed out, a truly infinite universe would contain, at any moment, infinite numbers of worlds identical to ours, some moving through time precisely as ours does, others with variations—in fact, all possible variations.</p>
<p>Again, compared to the whole of this Infinite Universe, and, we might also say, in the eyes of its Creator, the histories of individual worlds within it, along with their systems of morality and meaning, should be of infinitesimal significance. If we could take a God’s-eye view, zooming out from our planet to encompass our whole galaxy, and then galaxy clusters, and clusters of clusters, we would see the histories of individual worlds much as the video game player sees our world: less as sets of interlinked events, and more as manifestations of a broader, stochastic process, whose function is essentially only <em>to ink over the space of possibility</em>.</p>
<p>Contemporary physics, specifically quantum mechanics, delivers us to an even colder, darker destination. Quantum mechanics has at its core an equation, the Schrödinger wave equation, that implies a weird multiplicity of states for any given quantum-scale particle (an electron, for example) traveling through time. Physicists in the early years of quantum theory clung to the belief that these multiple states somehow probabilistically “collapse” to one state whenever one tries to observe the particle with a measuring device. However, in the past few decades the field basically has abandoned that rather hand-waving interpretation, mostly in favor of a simpler, more parsimonious one: that the multiple possible states a particle can be observed to have are all, in a sense, <em>real</em>.</p>
<p>In other words, these alternate states represent multiple actual particles existing in different “worlds” or “universes.” Thus, a physicist recording the impact of one particular state of an electron has, at that moment, otherwise identical counterparts in otherwise identical alternate universes who record the impacts of all the other states.</p>
<p>The reality implied by this interpretation—now called the Many Worlds Interpretation (MWI)—encompasses not just one very big universe but, rather, an infinite number of them, a “multiverse,” across which everything that can happen does happen. There is a perfection here that, at least in a technical sense, should impress those who always believed Creation would be flawless and complete.</p>
<p>Of course, from the usual sentimental human perspective, MWI looks bizarre and horrifying. Even so, its superior simplicity and parsimony, as a way of thinking about quantum phenomena, has enabled it to survive and spread despite its implications—which physicists don’t “like” any more than you or I do.</p>
<p>As Greene has <a href="https://www.amazon.com/Hidden-Reality-Parallel-Universes-Cosmos/dp/0307278123">noted</a>:</p>
<p style="padding-left: 40px;">I find it both curious and compelling that numerous developments in physics, if followed sufficiently far, bump into some variation on the parallel-universe theme.</p>
<p style="text-align: center;">*</p>
<p>“The multiverse will drive you crazy if you really think about how it affects your life, and I can’t live like that,” the philosopher of physics and MWI theorist Simon Saunders once told a <a href="https://www.newscientist.com/article/mg19526223-700-parallel-universes-make-quantum-sense/">reporter</a>. “I’ll just accept [it] and then think about something else, to save my sanity.”</p>
<p>Is <em>thinking about something else </em>a viable strategy to escape the psychological consequences of modern cosmology?</p>
<p>Conceivably it is, up to a point. Humans evolved with basic, powerful drives towards survival and procreation, and even religiosity; they thus probably have, on average, a significant innate resistance to nihilist worldviews. Even now, well into the third millennium A.D., most of the human population professes belief in one religion or another. Also, obviously, the average person has no deep understanding of, or interest in, MWI or other modern cosmological theories.</p>
<p>Yet the things we do learn and think about ultimately affect our behavior, if only subconsciously. One doesn’t have to be a philosopher or a psychologist to understand—to take another extreme example—that if we all knew our solar system would be obliterated within a year, making it obvious that our existence was and always had been inconsequential, enough of us would fall into despair that our societies would start to disintegrate immediately.</p>
<p>I think the reason we’ve largely been able, so far, to resist the toxic implications of modern cosmology is simply that we haven’t been forced to confront them. But that situation is changing.</p>
<p>When I was growing up in the 1970s and early 80s, cosmology was expansive but still quite tame compared to what was coming. Carl Sagan’s 1980 <em>Cosmos</em> TV series on PBS, for example, was hardly despair-inducing. One could contemplate the large universe depicted by Sagan and other pop cosmologists of the time, and, as I noted above, could still fantasize about humans’ someday traversing and conquering it. MWI and other infinite-universe theories had not yet caught on, certainly not at the popular level.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-764" src="https://thoughtsofstone.github.io/wp-content/uploads/2023/01/sagan-cosmos.jpg" alt="" width="714" height="440" /></p>
<p>These days, by contrast, MWI and similar “parallel universe” themes are essential elements of pop cosmology, and, perhaps more importantly, are also common in <a href="https://en.wikipedia.org/wiki/Everything_Everywhere_All_at_Once">pop culture</a> generally.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-765" src="https://thoughtsofstone.github.io/wp-content/uploads/2023/01/everything-ev.jpg" alt="" width="800" height="399" srcset="https://thoughtsofstone.github.io/wp-content/uploads/2023/01/everything-ev.jpg 800w, https://thoughtsofstone.github.io/wp-content/uploads/2023/01/everything-ev-768x383.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" /></p>
<p>Moreover, although technologies based on quantum mechanics (such as lasers) have been around for decades, newer quantum tech such as quantum computing and quantum encryption emphasizes, for the first time, the spookier, multiplicity-of-states aspect of quantum mechanics—the aspect that MWI essentially was devised to explain. Thus, from popular science to tech to popular media culture generally, people are being exposed to the infinite-universe/multiverse idea as never before, and in ever-stronger doses.</p>
<p>The impact of that rising exposure won’t be immediately obvious. There are, and in the coming decades will continue to be, many other drivers of despair, disruption, suicide, and social disintegration in the modern world—drivers such as <a href="https://thoughtsofstone.github.io/cultural-feminization-an-introduction/">cultural feminization</a>, mass immigration, and <a href="https://thoughtsofstone.github.io/the-ouroboros-economy/">human-displacing AI systems</a>. Trying to disentangle the effect of one of these from the others is going to be challenging, to put it mildly. But, if my hypothesis is correct, “cosmological despair” will weigh more and more heavily and evidently on developed societies—especially among younger people, who will encounter MWI and similarly harsh cosmologies in their formative years, never having had the comforts of older, friendlier worldviews. In other words, if the world is now entering an Age of Despair principally for other reasons, cosmology will keep it there terminally.</p>
<p style="text-align: center;">*</p>
<p>There probably aren’t very many clear examples, yet, of people taking their own lives as a result of belief in MWI or other toxic cosmologies. However, something like this seems to have happened in the case of Hugh Everett III—the physicist who developed the original version of MWI (“<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwieh-2-sbb8AhXvNlkFHcDLBqwQFnoECAkQAQ&amp;url=http%3A%2F%2Fwww.weylmann.com%2Frelative_state.pdf">’Relative State’ Formulation of Quantum Mechanics</a>”) as his Princeton PhD thesis in 1956.</p>
<p>Everett eventually became a financially successful tech entrepreneur and, in most ways seemed normal, being married with children, having friends, and pursuing ordinary hobbies and pleasures that included wine-making and ocean liner cruises. However . . .</p>
<p style="padding-left: 40px;">Everett firmly believed that his many-worlds theory guaranteed him immortality: His consciousness, he argued, is bound at each branching to follow whatever path does not lead to death—and so on ad infinitum. [<a href="https://space.mit.edu/home/tegmark/everett/everett.html#e24">link</a>]</p>
<p>Probably at least partly due to this belief, he smoked, drank, and ate with abandon, which ultimately gave him a <a href="https://www.washingtonpost.com/archive/local/1982/07/23/dr-hugh-everett-iii-founder-of-data-firm/16fc45d5-0e5e-445e-9714-12550bb6354e/">fatal heart attack</a> in 1982, when he was only 51 years old. In accordance with his wishes, his body was cremated and his ashes were thrown out with other household garbage.</p>
<p>A decade and a half later, Everett’s troubled 39-year-old daughter Liz took her own life even more directly. She left a note to the effect that she wanted her own ashes thrown out with the garbage, so that she might “end up in the correct parallel universe to meet up w[ith] Daddy.”</p>
<p>&nbsp;</p>
<p style="text-align: center;">***</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>THE OUROBOROS ECONOMY</title>
		<link>https://thoughtsofstone.github.io/the-ouroboros-economy/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Fri, 02 Dec 2022 04:32:55 +0000</pubDate>
				<category><![CDATA[economics]]></category>
		<category><![CDATA[fall of the West]]></category>
		<category><![CDATA[human ecology]]></category>
		<category><![CDATA[immigration]]></category>
		<category><![CDATA[psychology]]></category>
		<category><![CDATA[technology]]></category>
		<category><![CDATA[USA]]></category>
		<guid isPermaLink="false">https://thoughtsofstone.github.io/?p=730</guid>

					<description><![CDATA[You are about to become obsolete. &#160; When labor becomes scarce, expensive, and/or unreliable, business owners start looking for alternatives. For most of the past 30 years, a very attractive alternative was offshoring—to countries like China, where labor was cheap, plentiful, and reliable. In the past three years, the COVID pandemic and the maturing of &#8230; <a href="https://thoughtsofstone.github.io/the-ouroboros-economy/" class="more-link">Continue reading<span class="screen-reader-text"> "THE OUROBOROS ECONOMY"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>You are about to become obsolete.<br />
</em></p>
<p><span id="more-730"></span></p>
<p>&nbsp;</p>
<p>When labor becomes scarce, expensive, and/or unreliable, business owners start looking for alternatives. For most of the past 30 years, a very attractive alternative was offshoring—to countries like China, where labor was cheap, plentiful, and reliable. In the past three years, the COVID pandemic and the maturing of once-cheap labor markets, plus the increasing obviousness of China’s IP theft and overall hegemonic ambitions, have begun to reverse that trend. Economists are now forecasting “the end of globalization,” with labor scarcity <a href="https://www.conference-board.org/topics/recession/how-high-will-US-unemployment-go">continuing</a> for decades as the workforce shrinks. Big companies, desperate for workers, are even indicating a willingness to hire people <a href="https://www.wsj.com/articles/employers-rethink-need-for-college-degrees-in-tight-labor-market-11669432133">without college degrees </a>for positions that traditionally required them.</p>
<p>To me, though, the idea that labor will continue to be scarce seems wrong. As I see it, mechanization and AI are now moving onto the steepest part of the innovation slope, and will soon start “disemploying” people all the way up the labor value chain, from manual trades to those overpaid millennial marketing girls sipping lattes on TikTok. Even I, with my fairly challenging profession and decades of experience, am likely to be left jobless at least a few years before I’d like to retire.</p>
<p>AI has taken longer than expected to arrive in useful forms, but is now definitely arriving and ready to start disrupting. It can, technically if not yet legally, drive cars, tractors, trains, and boats; fly planes and drones; and guard warehouses. The mechanization technology underlying humanoid robots has been making big advances too—such robots now can open doors, climb stairs, recover from falls, hold and manipulate heavy objects, etc. Once such robots are mass-produced and made available for leasing, their use as replacements for factory workers, waiters, construction workers, checkout clerks, etc. will become a viable proposition. Will we have to wait as long as five years before that starts?</p>
<p>AI language-processing software that can be taught, or can teach itself via the Internet, should start displacing office worker bees well before then—and by worker bees I mean basically anyone whose job consists largely of emailing, writing reports, filling out spreadsheets, and doing other routine kinds of paperwork. And we’ve all seen the AI text-to-image and text-to-movie <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">packages</a> that were unleashed recently and have been improving at a rapid pace. How long will it be before a single writer, working with one of those algos, generates a feature-length film on his own? A year from now? Two?</p>
<p>Essentially, we’re facing the prospect of the abrupt end of the labor market, an institution that has been at the center of human civilization for millennia.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-732" src="https://thoughtsofstone.github.io/wp-content/uploads/2022/12/silent-running.jpg" alt="" width="1200" height="638" srcset="https://thoughtsofstone.github.io/wp-content/uploads/2022/12/silent-running.jpg 1200w, https://thoughtsofstone.github.io/wp-content/uploads/2022/12/silent-running-768x408.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" /></p>
<p>I’m aware that critics of earlier forms of labor-saving technology, such as the Luddites and their ilk, were somewhat shortsighted in their predictions of mass labor displacement. These early industrial workers were themselves displaced in large numbers, and in that sense had every reason to complain. What they failed to see was that the labor-saving innovations that displaced them would, on the whole, lead to greater productivity and economic growth, and ultimately a net rise in demand for labor.</p>
<p>But that was then, and this is now. The tech that’s going to be released into the world in this decade will be capable of displacing humans from their jobs much faster than the latter will be able to keep up. In other words, if you are laid off because your employer or clients can just buy an AI package to do the same job more cheaply, and you then decide to retrain for some “AI-proof” job, it’s quite likely that that “AI-proof” job will be overtaken by AI long before you can get into it. Even if that job stays available, you’d be competing for it with an exponentially rising number of other displaced human workers.</p>
<p>It’s impossible to predict in detail how this will all play out. But I can easily imagine an early phase in which language-processing AI, vehicle AI, warehouse robots, and a few other related innovations are hailed as game-changers for businesses and other organizations, allowing them to do much more with fewer workers and at less cost—and alleviating inflationary labor shortages along the way. Close on the heels of that “denial” phase, though, will come the bargaining, depression, and acceptance phases, as the pace of disemployment accelerates. I see this as an ouroboros—snake-eating-its-tail—process, because it involves the economy effectively consuming itself, i.e., destroying, with every increment of growth and investment in innovation, the employment earnings that are the principal fuel for a modern economy.</p>
<p>There may be no stable equilibrium in this process for a long while. Governments probably will try to tax businesses, especially AI-using businesses, to fund welfare payments to the unemployed masses, but will that work? Even if governments could manage it fiscally, what would be the psychological effect on tens of millions of people who can no longer earn a living for themselves? (We already know that <a href="https://blogs.lse.ac.uk/politicsandpolicy/men-are-more-likely-to-suffer-adverse-health-consequences-as-a-result-of-unemployment-than-women/">men become easily depressed when unemployed</a>.)</p>
<p><img decoding="async" class="aligncenter size-full wp-image-734" src="https://thoughtsofstone.github.io/wp-content/uploads/2022/12/wall-e.jpg" alt="" width="790" height="331" srcset="https://thoughtsofstone.github.io/wp-content/uploads/2022/12/wall-e.jpg 790w, https://thoughtsofstone.github.io/wp-content/uploads/2022/12/wall-e-768x322.jpg 768w" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" /></p>
<p>It also seems unlikely that Western governments would ever simply disallow the use of AI and robotics. One of the great lessons of the mass immigration era is that Western governments, ostensibly “democratic,” <em>like</em> having electorates made up of financially stressed people whose votes can be bought with government largesse.</p>
<p>It stands to reason that the disemployment situation will be easier in countries that currently have relatively small workforces—or rely on guest workers who can be sent quickly back to their home countries if needed. By the same logic, countries with open borders and huge, low-skill, permanent immigrant populations, like the US, could be in serious trouble. Those countries will suddenly have many millions of excess mouths to feed, and to do so might easily require taxation levels that trigger capital flight.</p>
<p>I’m not totally averse to the idea that at the end of this transition lies a society in which robots do everything for near-zero cost and humans can stay busy however they like without worrying much about money. But it’s hard to believe this transition will occur without historic levels of pain. I’ve written often in this space about various drivers of Western decline, collapse, and general upheaval; the now-imminent “ouroboros economy” of AI and robotics is surely another one.</p>
<p style="text-align: center;">***</p>
<p>&nbsp;</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>THE TREE OF KNOWLEDGE</title>
		<link>https://thoughtsofstone.github.io/the-tree-of-knowledge/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Sat, 16 Apr 2022 21:52:30 +0000</pubDate>
				<category><![CDATA[ideas]]></category>
		<category><![CDATA[psychology]]></category>
		<category><![CDATA[technology]]></category>
		<guid isPermaLink="false">https://thoughtsofstone.github.io/?p=566</guid>

					<description><![CDATA[Encourage new ideas and more productive dialogue by giving idea-originators the credit they’re due &#160; Late in 2011, I first put pen to paper, or fingertips to keyboard, to describe an idea that had been bouncing around in my mind for several years—at least since Larry Summers’s public defenestration from Harvard in 2005. The idea &#8230; <a href="https://thoughtsofstone.github.io/the-tree-of-knowledge/" class="more-link">Continue reading<span class="screen-reader-text"> "THE TREE OF KNOWLEDGE"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Encourage new ideas and more productive dialogue by giving idea-originators the credit they’re due</em></p>
<p><span id="more-566"></span></p>
<p>&nbsp;</p>
<p>Late in 2011, I first put pen to paper, or fingertips to keyboard, to describe an idea that had been bouncing around in my mind for several years—at least since Larry Summers’s public <a href="https://thoughtsofstone.github.io/the-day-the-logic-died/">defenestration</a> from Harvard in 2005. The idea was that women’s unprecedented mass entry into public life, in the US and other Western societies, had been “feminizing” public discourse and policy—leading among other things to the rise of political correctness culture, the increasing suppression of free speech, and the decline of capital punishment—due to women’s different way of thinking about the world.</p>
<p>I wasn’t a professional opinionator—just an amateur with a background in journalism. But I did have a website where I published occasional essays, which were read by a few hundreds to thousands of people every month, and there I first set down my thoughts in a <a href="https://james-the-obscure.github.io/the-demise-of-guythink/">short piece</a> about the demise of the male cognitive style. In the ensuing decade, I developed the basic idea in further essays, including in two web magazines with significant readership. I joined Twitter to promote these essays. Eventually I had some recognition as an introducer of this idea.</p>
<p><img decoding="async" class="size-full wp-image-573 aligncenter" src="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/spotted-toad.jpg" alt="" width="591" height="224" srcset="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/spotted-toad.jpg 591w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/spotted-toad-300x114.jpg 300w" sizes="(max-width: 591px) 85vw, 591px" /></p>
<p><img decoding="async" class="wp-image-574 aligncenter" src="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/cowen-Copy.jpg" alt="" width="471" height="310" srcset="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/cowen-Copy.jpg 660w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/cowen-Copy-300x198.jpg 300w" sizes="(max-width: 471px) 85vw, 471px" /></p>
<p><img decoding="async" class="wp-image-575 aligncenter" src="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/pink-shift-cover.jpg" alt="" width="364" height="527" srcset="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/pink-shift-cover.jpg 1246w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/pink-shift-cover-207x300.jpg 207w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/pink-shift-cover-707x1024.jpg 707w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/pink-shift-cover-768x1113.jpg 768w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/pink-shift-cover-1060x1536.jpg 1060w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/pink-shift-cover-1200x1738.jpg 1200w" sizes="(max-width: 364px) 85vw, 364px" /></p>
<p>The idea of cultural feminization became more and more obvious in the wake of the Great Awokening, which appeared to be a heavily female-dominated social phenomenon. One <a href="https://thecritic.co.uk/issues/december-january-2022/new-female-ascendency/">writer</a> after <a href="https://www.nytimes.com/2022/01/12/opinion/gender-gap-politics.html">another</a> began to notice and embrace the cultural feminization idea as their own—either not knowing of my (and others’) prior <a href="https://thoughtsofstone.github.io/cultural-feminization-a-bibliography/">contributions</a>, or knowing of them but not considering them “big” enough, in terms of readership and public awareness, to acknowledge them.</p>
<p>I may be ultra-sensitive on this topic, because some of my past journalistic work has been used by others with inadequate attribution (or none). I am also aware that this is a very personal kind of pain, one that tends to attract little sympathy from those who have not had a similar experience.</p>
<div style="width: 640px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-566-1" width="640" height="352" preload="metadata" controls="controls"><source type="video/mp4" src="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/Dustin-Hoffman-And-Robert-De-Niro-Wag-The-Dog-I-Want-The-Credit-2-1-Copy.mp4?_=1" /><a href="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/Dustin-Hoffman-And-Robert-De-Niro-Wag-The-Dog-I-Want-The-Credit-2-1-Copy.mp4">https://thoughtsofstone.github.io/wp-content/uploads/2022/04/Dustin-Hoffman-And-Robert-De-Niro-Wag-The-Dog-I-Want-The-Credit-2-1-Copy.mp4</a></video></div>
<p>Still, I think there is an objective case to be made, wholly apart from my own gripes, that “inadequate credit to originators of ideas” (ICOI?) is a legitimate social problem whose solution would materially benefit society. Why? Simply because denying people credit for developing original ideas effectively disincentivizes them from making the effort to develop those ideas, and in that straightforward sense is likely to retard human progress. Obscuring past expressions of an idea also limits new discussions of that idea when they do occur—people are constantly “reinventing the wheel.”</p>
<p>We acknowledge this logic in other domains of knowledge production. The new ideas of scientists, doctors, English Lit experts, etc. are protected and made accessible by the searchability of their formal papers (via the PubMed system, for example) and the citation custom. New technical ideas also are protected and made accessible by the patent system and its strong judicial backing. Books, musical compositions, movies, TV shows, are protected—albeit weakly—by the copyright system.</p>
<p>Unfortunately, when it comes to ordinary, non-patentable, non-copyrightable ideas that are published in informal, non-academic fora—ideas such as “women’s mass entry into public life has feminized institutions, culture and policy”—there is no legal protection, and no good and searchable repository. Moreover, the principal search engines for web-based material aren’t really designed to uncover the origins and subsequent iterations of ideas. Mainly because of this, there is only a very weak cultural enforcement of priority claims by idea-originators. I observe anecdotally, for example, that Twitter is full of DIY experts who (from lack of knowledge and/or lack of scruples) recycle others’ ideas, framing their contributions as new, though they are unoriginal and are often incomplete/undeveloped compared to the true original. Thus, the true originators are discouraged by not getting credit, and the false ones lead everyone in circles instead of moving the discussion productively forward.</p>
<p><strong>Solutions?</strong></p>
<p>In a broad-brush way, I imagine two complementary solutions to this problem. One is to develop a “registry of ideas.” The other is to develop an “idea history” search engine/bot.</p>
<p>The registry of ideas could have a broad structure like the “<a href="https://en.wikipedia.org/wiki/Motif-Index_of_Folk-Literature">motif index</a>” developed by the anthropologist Stith Thompson in the 1930s to record and categorize folkloric tales according to their different elements or &#8220;motifs.”</p>
<p><img decoding="async" class="wp-image-569 aligncenter" src="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/motif-index-Copy-2-scaled.jpg" alt="" width="455" height="736" srcset="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/motif-index-Copy-2-scaled.jpg 1584w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/motif-index-Copy-2-186x300.jpg 186w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/motif-index-Copy-2-633x1024.jpg 633w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/motif-index-Copy-2-768x1241.jpg 768w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/motif-index-Copy-2-950x1536.jpg 950w, https://thoughtsofstone.github.io/wp-content/uploads/2022/04/motif-index-Copy-2-1267x2048.jpg 1267w" sizes="(max-width: 455px) 85vw, 455px" /></p>
<p>The Dewey Decimal System, by which librarians decide where to shelve books, and the aborted Google “<a href="https://dbpedia.org/page/Knol">Knol</a>” project, offer other potentially helpful models of topic-categorization.</p>
<p>I imagine that some organization like the Wikimedia Foundation (Wikipedia’s parent) could provide some initial structure and guidelines for such a database, after which it would be assembled and curated by volunteers in a highly distributed manner—like Wikipedia. Idea-originators could directly submit their work for recording in the registry, as they can now for appropriate intellectual property with the patent and copyright systems, or they could just let the curators detect their work (assuming it had been published) with their specially designed web bots and other search tools.</p>
<p>Those tools would be powered by “idea history” search algos, which would parse natural language text to identify ideas being expressed, and categorize them—at least as a “first pass” effort to be checked by human curators of different idea domains, though AI tech may soon be able to handle such tasks on its own.</p>
<p>I suppose one objection to this whole idea—this idea about ideas—would be that ordinary writers, especially casual self-published essayists and social-media posters, won’t submit to a citation system the way academics do. I think, though, that to a great extent, ordinary writers already <em>do</em> follow the custom of citation. These days (unlike, say, 50 years ago) a large majority of young adults in the US have had some college experience, with the exposure to citation rules that entails. More importantly, the internet with its “hypertext” ability that can easily link text to other documents anywhere on the web has made citation of others’ work routine even for the most casual writers. The only things missing, really, are the search engines and repositories that would help to fully routinize the assignment of priority, rewarding creativity appropriately, and plausibly making even our humdrum, everyday dialogue a bit more rigorous and serious.</p>
<p style="text-align: center;">* * *</p>
]]></content:encoded>
					
		
		<enclosure url="https://thoughtsofstone.github.io/wp-content/uploads/2022/04/Dustin-Hoffman-And-Robert-De-Niro-Wag-The-Dog-I-Want-The-Credit-2-1-Copy.mp4" length="1281012" type="video/mp4" />

			</item>
	</channel>
</rss>
