<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>A.I. &#8211; Thoughts of Stone</title>
	<atom:link href="/category/a-i/feed/" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>short essays, usually about humans</description>
	<lastBuildDate>Fri, 23 Jun 2023 22:47:52 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.2.2</generator>

<image>
	<url>/wp-content/uploads/2020/07/cropped-icon-32x32.jpg</url>
	<title>A.I. &#8211; Thoughts of Stone</title>
	<link>/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>MISTRESSES OF MISRULE</title>
		<link>/mistresses-of-misrule/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Thu, 22 Jun 2023 23:34:30 +0000</pubDate>
				<category><![CDATA[A.I.]]></category>
		<category><![CDATA[ethnicity]]></category>
		<category><![CDATA[fall of the West]]></category>
		<category><![CDATA[freedom]]></category>
		<category><![CDATA[freedom of speech]]></category>
		<category><![CDATA[immigration]]></category>
		<category><![CDATA[psychology]]></category>
		<category><![CDATA[wokeism]]></category>
		<category><![CDATA[women]]></category>
		<guid isPermaLink="false">/?p=827</guid>

					<description><![CDATA[Women and civilizational collapse &#160; Complaints of “toxic” workplaces. Mass hiring of diversity-equity-inclusion commissars. Open-borders immigration sold to the public with tear-jerking images of refugee children. Trans mania spreading everywhere from kindergarten classrooms to corporate C-suites. Personal pronouns in work email signatures. White women kneeling in prayerful mass protests after yet another African-heritage male with &#8230; <a href="/mistresses-of-misrule/" class="more-link">Continue reading<span class="screen-reader-text"> "MISTRESSES OF MISRULE"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Women and civilizational collapse</em></p>
<p><span id="more-827"></span></p>
<p>&nbsp;</p>
<p>Complaints of “toxic” workplaces. Mass hiring of diversity-equity-inclusion commissars. Open-borders immigration sold to the public with tear-jerking images of refugee children. Trans mania spreading everywhere from kindergarten classrooms to corporate C-suites. Personal pronouns in work email signatures. White women kneeling in prayerful mass protests after yet another African-heritage male with a mile-long rap sheet resists a cop and is shot. Removal of traditional due-process rules to favor women’s sex claims. Talk of “reparations” on a trillion-dollar scale, to remedy racial inequalities. Ever-stricter limits on acceptable speech, debate and scientific inquiry. Declining support for truth itself, if the truth might cause hurt feelings. A heavy emphasis on trauma and victimhood in news media, literature, law, and psychiatry. Open governmental discrimination against white males.</p>
<p>All these are manifestations of a societal climate change that has been underway since the 1950s, with a sharply increased pace in recent years. The causes are many, but one is more important than all the rest put together. I am referring to <a href="/the-great-feminization/">the entry of women into public life</a>, which—particularly in recent decades as women have ascended to the upper ranks of all important institutions—has given them unprecedented cultural and political power.</p>
<p>I’ve been <a href="/the-day-the-logic-died/">writing </a>about this for more than a <a href="https://james-the-obscure.github.io/the-demise-of-guythink/">decade</a>. During most of that time, my hypothesis was rejected or ignored, presumably because it was considered too heretical. In the past year and a half, other more prominent figures have started to write about some of the particular institutional effects of women’s new power (e.g., <a href="https://thecritic.co.uk/issues/december-january-2022/new-female-ascendency/">on academia</a>), presumably in part because it has become just too obvious to ignore. What I think is still being missed—or suppressed—is the true extent of this process of cultural feminization, and, more importantly, the disastrous future towards which it is driving.</p>
<p><strong>The ubiquity of cultural feminization</strong></p>
<p>Women’s new power is being wielded, and felt, not just in the universities, not just in H.R. offices, not just among mainstream media corporations and big publishing houses, not just among millennials, but <em>everywhere</em>, affecting everyone. It is what I have called a general “<a href="https://americanmind.org/salvo/pink-shift/">pink shift</a>” in the culture. The fact that even traditionally macho institutions such as the military and sports leagues have been <a href="https://www.newsweek.com/wokeism-hurting-military-recruitment-congressman-warns-1807962">affected</a> is a good indication of its power and breadth.</p>
<p>Women’s ascension to power in institutions, and in public life generally, has altered the culture for the simple reason that women, <em>on average</em>, do not think and act as men do. They are more emotionally sensitive and compassionate, more tuned into people and direct relationships rather than abstract rules and systems and hierarchies. They are quicker to form and join social networks, and to spread social contagions. They are more risk-averse, less interested in conquest and exploration, and more sensitive to environmental threats. They have less tolerance for the stressful combat of free debate, less respect for scientific inquiry for its own sake, less patience with the idea of judicial due process. Probably as a result of being more emotionally sensitive, they seem more easily influenced by narratives that emphasize short-term, emotion-evoking consequences, and seem less interested in dry analyses of long-term outcomes. Perhaps especially when they are childless (or their children have “left the nest”), they are more likely to embrace the “disadvantaged” of the world as their virtual children, feeling emotional pain at persistent inequalities among them, and seeking to alleviate that pain by almost any means necessary.</p>
<p>Of course, women differ among themselves in the strengths of their psychological traits, as do men. But the basic idea here is that the two sexes’ overlapping “bell curves” of trait distributions have significantly different averages or means, which I think is evident even on small, organizational scales, but is seismically obvious on a civilization level.</p>
<p>In short, women collectively have their own distinct perspective on the world, and, now that their power exceeds men&#8217;s, they are showing their disdain for the world men made, declaring: “We can do better.”</p>
<p>But <em>can</em> they do better? And why is this important question missing from Western public discourse?</p>
<p><strong>Hiding their power</strong></p>
<p>I had trouble getting my earlier essays on cultural feminization published even in smaller, decidedly conservative media. I can’t be absolutely certain of the reasons, but, as everywhere else in media, there were always female editors in the decision chain—often at the top—and of course thousands of female subscribers who might be angered by anything frame-able as “anti-women.”</p>
<p>The idea that women have unprecedented cultural power, and with it have been dramatically reshaping most of the world’s societies, is, of course, not inherently anti-women. Why can’t women just accept their triumph and take a victory lap? Why does there appear to be not just an overlooking of this historic social phenomenon but even (apart from a few opinion pieces) a sort of conspiracy of silence about it, especially among women?</p>
<p>One explanation is obvious. Women as the physically weaker, more risk-averse sex have traditionally wielded power less openly and directly. As such, they tend not to want to reveal their power, let alone crow over it; they prefer to emphasize their weakness and chronic victimization—which, among other effects, triggers a protective reflex among many men.</p>
<p>I don’t think that’s a complete explanation, though. I think that women like to hide their power not only because it’s more effective when hidden, but also because they realize, deep down, that female supremacy is hard to defend as an optimal way of steering civilization.</p>
<p>Even the feminist who openly seeks absolute female power—the kind of woman who asks “why do we need men?”—is well aware of (has “internalized”) the traditional, disparaging view of the female mindset. This is the view (one might call it the <a href="https://penelope.uchicago.edu/aristotle/histanimals9.html">Aristotelian view</a>, though it has been expressed by modern women as different as Ann Coulter and Camille Paglia) that women, relative to men, are irrational, flighty, suggestible, overly emotional, unstable, given to herd thinking, and prone to hysterias and other social contagions. And although this traditional view may seem crude and unfair, most women at least understand that there really is such a thing as the “female mindset,” that it does involve greater emotional sensitivity and people-centeredness in most situations, and that it makes women better mothers than they would be if they were more male-brained.</p>
<p>But is this female mindset somehow superior to the traditional male mindset when it comes to shaping culture and policy? I have never seen or heard a woman make this claim explicitly, probably because the weakness of the claim is obvious. Why would a female, maternal mindset be superior in the public sphere, when it is an adaptation for a very different environment, i.e., actual maternity, which in fact has occurred traditionally within the protective bounds of male-managed society? By the same token, why would the male mindset be <em>inferior</em> when it must be, at least in part, an adaptation for the public sphere—where men have reigned from the dawn of hominids?</p>
<p>It seems to me that women, having no solid argument to justify their cultural and political ascendancy (“it’s our turn” “men are toxic”), and knowing that debates in general play to male strengths, have decided simply to avoid the issue by pretending their ascendancy hasn’t occurred.</p>
<p><strong>Female empowerment leads to social collapse</strong></p>
<p>Not every social change driven by this “<a href="https://thoughtsofstone.github.io/the-great-feminization/">Great Feminization</a>” process has been adverse, but it does seem that most have—and that the net effect is increasingly dystopian.</p>
<p>These bad consequences also seem very predictable, at least from a male perspective.</p>
<p>Some examples:</p>
<p style="padding-left: 40px;"><u>New, lenient policing and sentencing laws</u>.</p>
<p style="padding-left: 40px;">Short-term goal: Stop police oppression of African-Americans.</p>
<p style="padding-left: 40px;">Long-term effects: Incentivization of law-breaking, rampant crime, business flight.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-829" src="/wp-content/uploads/2023/06/shoplift.jpg" alt="" width="445" height="273" /></p>
<p style="padding-left: 40px;"><u>Municipal laws that prevent removal of homeless and other street people, offer food etc.</u></p>
<p style="padding-left: 40px;">Short-term goal: Treat homeless people with compassion.</p>
<p style="padding-left: 40px;">Long-term effects: Incentivization of homelessness, filthy encampments that spoil large areas of the city, more crime, business flight.</p>
<p style="padding-left: 40px;"><u>Generous welfare policies</u></p>
<p style="padding-left: 40px;">Short-term goal: Treat the disadvantaged with compassion, reduce hunger, etc.</p>
<p style="padding-left: 40px;">Long-term effects: Incentivization of indigency, spread of welfare dependency, impairment of family-formation (mothers lose incentive to marry), plus all the social pathologies that follow from these.</p>
<p style="padding-left: 40px;"><u>Promotion of anti-traditional behaviors/lifestyles (homosexuality, transsexualism)</u></p>
<p style="padding-left: 40px;">Short-term goal: Empower the marginalized.</p>
<p style="padding-left: 40px;">Long-term effects: Weakening of social norms, spread of what is effectively antisocial (anti-family) behavior, spread of associated mental illness in the most impressionable, i.e., children and young adults.</p>
<p style="padding-left: 40px;"><u>Opposition to restrictions on immigration</u></p>
<p style="padding-left: 40px;">Short-term goal: Help the “huddled masses” (i.e., the same maternal sentiments expressed in Emma Lazarus’s famous <a href="https://en.wikipedia.org/wiki/The_New_Colossus">sonnet</a>.)</p>
<p style="padding-left: 40px;">Long-term effects: Incentivization of mass/illegal immigration. Destruction of national identity, lowering of trust, increase in despair, price inflation, brain-drain in origin countries, etc.</p>
<p style="padding-left: 40px;"><u>Restriction of speech, debate, legal due-process, scientific inquiry</u></p>
<p style="padding-left: 40px;">Short-term goal: Prevent the emotional turmoil caused by “hateful” arguments, concepts, or simple observations, e.g., of racial differences in cognitive and behavioral traits.</p>
<p style="padding-left: 40px;">Long-term effects: Destruction of liberal norms, a maternal “because I said so!” illiberalism, corruption of scientific culture, reversal of scientific progress.</p>
<p style="padding-left: 40px;"><u>Promotion of equal outcomes vs. equality of opportunity</u></p>
<p style="padding-left: 40px;">Short-term goal: Reduce conflict and promote fairness by directly reducing financial inequality (resembling a classic maternal strategy for promoting harmony among children—also probably the norm in family-based paleolithic groupings)</p>
<p style="padding-left: 40px;">Long-term effects: Destruction of normal, healthy incentives to succeed. Promotion of lazy, redistributive attitude (“I’m a victim of racism—give me money”). A centerpiece of communism/socialism and a key reason for its failure.</p>
<p style="padding-left: 40px;"><u>Promotion of “harm reduction” strategies (e.g., free needles) against illicit drug use</u></p>
<p style="padding-left: 40px;">Short-term goal: Reduce mortality and hospitalizations due to drug overdoses.</p>
<p style="padding-left: 40px;">Long-term effects: Incentivization of drug use.</p>
<p>The overall pattern should be clear: The feminine mindset, with its focus on short-term, feelgood outcomes in the culture and policy realm, tends to set up perverse incentives, thereby basically guaranteeing bad <em>long-term</em> outcomes.</p>
<p>Incidentally, the psychologist Simon Baron-Cohen has famously argued, with experimental evidence, that the “female brain,” compared to the “male brain,” is less good at understanding and building systems. It is easy to see why this would be so, if the feminine mindset is relatively blind to the mechanisms that determine a system’s long-run performance—the system in question here being the system of humans called society.</p>
<p>Women’s greater focus on the emotional and the short-term has other adverse impacts on culture and policy. One is the “witch-hunt,” social-contagion-prone atmosphere that now suffuses Western (esp. Anglo-American) culture—and I think derives from the heightened feminine sensitivity to the stress of debate (including greater pain from the cognitive dissonance generated by opposing arguments), and the broader feminine need for emotional harmony in groups. The speed with which women, led by their woke high priestesses, have been dismantling Western traditions in favor of fads and frenzies such as “gender-affirming care for children,” is stunning and ominous.</p>
<p>Even more ominous, though, is the weakness of public opposition, which, of course, is due largely to women’s reluctance even to acknowledge their power, let alone restrain its excesses.</p>
<p>Will the West continue to collapse by a slow process of social dissolution? It’s easy to picture that happening simply as a continuation of trends our cultural matriarchy promotes: Third-World-ization via immigration, white self-hatred, discrimination against men, low Western fertility, diversity over merit, sanctioned lawlessness for protected racial groups, etc. It’s also plausible that the collapse will be more sudden and catastrophic, via, say, lost wars, surrenders to invader-immigrants who are not so feminized (or so civilized), or even, one day, the sentimental granting of civil rights to &#8220;sentient&#8221; machines. Anyway, as far as I can see, all paths in our feminized civilization lead to the failure of that civilization. It’s almost beside the point to note that that failure will bring this brief, strange period of female cultural hegemony to a close.</p>
<p style="text-align: center;">* * *</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>FROM TECH FREEDOM TO WORLD GOVERNMENT</title>
		<link>/from-tech-freedom-to-world-government/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Sun, 05 Mar 2023 00:40:38 +0000</pubDate>
				<category><![CDATA[A.I.]]></category>
		<category><![CDATA[freedom]]></category>
		<category><![CDATA[technology]]></category>
		<guid isPermaLink="false">/?p=788</guid>

					<description><![CDATA[Further notes on the “unaligned A.I.” problem &#160; A lot of dust is now being raised by media hype and corporate positioning about A.I.—similar to what we saw in the early days of the Internet. Behind all the dust clouds, though, there’s an active debate among techies and tech-adjacent types about the “A.I. apocalypse” that &#8230; <a href="/from-tech-freedom-to-world-government/" class="more-link">Continue reading<span class="screen-reader-text"> "FROM TECH FREEDOM TO WORLD GOVERNMENT"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Further notes on the “unaligned A.I.” problem</em></p>
<p><span id="more-788"></span></p>
<p>&nbsp;</p>
<p>A lot of dust is now being raised by media hype and corporate positioning about A.I.—similar to what we saw in the early days of the Internet. Behind all the dust clouds, though, there’s an active debate among techies and tech-adjacent types about the “A.I. apocalypse” that may lie in our future.</p>
<p>My <a href="/yudkowskys-golem/">previous post</a> has more details, but anyway I’m referring to a future in which A.I. systems will be significantly more powerful than they are today—maybe capable of running entire industries, maybe capable of running everything. While these systems could displace most/all humans from the production side of the economy, they could also drive the costs of goods and services so low that anyone, on the strength of savings or a state subsidy, could live a comfortable life. (In other words, the “paradise” depicted in films like <em>Wall-E</em>.) One catch is that these A.I. systems, if built with the same machine-learning design approaches used in modern ChatGPT-type systems, effectively will be advanced non-human intelligences with opaque cognitive processes. It might be as hard, or even harder, to train them to “align” their values with human values as it is now with much more primitive systems. That’s a problem because an <em>unaligned</em> A.I. is one that plausibly would have no compunction about doing away with humans—just as soon as it could <a href="/yudkowskys-golem/">survive without them</a>.</p>
<p><iframe title="159 - We’re All Gonna Die with Eliezer Yudkowsky" src="https://www.youtube.com/embed/gA1sNLL6yg4" width="800" height="550" frameborder="0" allowfullscreen="allowfullscreen"><span style="display: inline-block; width: 0px; overflow: hidden; line-height: 0;" data-mce-type="bookmark" class="mce_SELRES_start">﻿</span></iframe></p>
<p>We already know that current AIs are capable of pretty <a href="https://stratechery.com/2023/from-bing-to-sydney-search-as-distraction-sentient-ai/">weird and unfriendly</a> behavior. We know their mindset is inhuman and inherently difficult to train to do useful things while also obeying moral rules. We know we have no robust, foolproof way to instill a “do not harm people” principle in them. It really is believable that one or more of them, when cognitively scaled up and given the opportunity, would try to exterminate some or all of us, as casually as you or I would spray Raid on some ants we had found in the kitchen.</p>
<p>Many A.I. and “A.I. ethics” experts are thinking about this problem now. At least one prominent researcher, Eliezer Yudkowsky, has rather emotionally thrown his hands up in despair (see video above). He will keep thinking about the alignment problem, he says, but for now has no good solution—and worse, has no confidence in the folks that currently control A.I. research.</p>
<p>My own view, fwiw (I’m not an A.I. expert though I have a technical background), is that the A.I. alignment problem <em>isn’t</em> the main problem here.</p>
<p>Alignment <em>should</em> be a soluble technical problem for an A.I. system if its architecture is designed with the need for alignment in mind. A key goal of this design approach would be to ensure that the A.I.’s motives and specific plans are always transparent. It’s like putting a speed governor on a car’s drive system—a relatively straightforward task, if you have a real-time readout from an accurate speedometer.</p>
<p>There is a deeper problem, though—a deeper problem that is also a general problem in societies that believe their cultures and technologies should be free to evolve where they will. Put simply, although many technologies have potentially hazardous side-effects, in Western societies hardly any of them are regulated so strongly that their hazards are effectively mitigated in every instance of the technology.</p>
<p>In the case of A.I., it should be technically possible, maybe even easy, to align <em>a given system </em>with training/hard-coding, assuming it has the right architecture. Enforcing the alignment of <em>every</em> A.I. system that presents a potential hazard, in order to cut the risk to zero, would be the real challenge. Even domestic enforcement would be tough, but international enforcement—against bad-actor states like Russia, China, and North Korea—could be impossible without war-like cross-border interventions. And, again, we’re not talking about a technical issue of A.I. design. We’re talking about the geopolitical issue of being able to control, regulate, and, if needed, destroy other countries’ A.I.s.</p>
<p>It’s easy to imagine that as A.I. develops in Western countries, domestic regulatory regimes will develop around it, perhaps modeled on existing regulatory systems covering nuclear reactors and the plutonium and other radioactive byproducts they generate. (The antiterrorism model is probably also applicable.) For the regulation of “foreign A.I.s,” the system will probably resemble the modern arms control and anti-proliferation setup.</p>
<p>Modern arms control and antiproliferation efforts, so far, have been moderately successful in keeping nukes out of the hands of crazy states. Obviously, they have not been <em>entirely</em> successful: see Iran, Pakistan, N. Korea. Moreover, A.I. could be a lot harder to regulate than nuclear weapons. Nukes require very special materials and engineering knowledge. By contrast, even a future superintelligent A.I., in principle, might be able to use consumer-grade hardware that any moderately wealthy Dr. No type could obtain from Amazon.com and assemble undetectably on private property. Most importantly, the hazard from any instance of an advanced A.I. is potentially infinite from the human perspective, whereas the hazard from any single nuclear weapon (or even all of them) is much more limited.</p>
<p>So a plausible scenario is that Western and Western-allied governments will set up A.I. regulatory systems domestically, and, to the extent they can, a regulatory/antiproliferation system abroad. Presumably they will also take steps to counter or survive against specific WMD threats from A.I.s gone bad—threats that could really run the gamut of nightmares, including totally novel pathogens with human-exterminating potential. Despite all this effort, though, it seems unlikely that “the good guys” will be able to mitigate the risk sufficiently within the system of nations that now exists.</p>
<p>On the other hand, as the awareness of the risk grows (possibly due to actual disasters), it should push Western governments to work together more and more tightly, to do whatever they can to extend A.I. regulation—<em>coercively,</em> if necessary—to non-compliant individuals and organizations in the West, and to entire non-compliant countries outside the West. If the risk is as big, and as hard-to-mitigate, as I suspect, then the end result could be effectively a single, highly intrusive, all-surveilling World Government. Obviously, the risks from other hazardous techs will tend to drive things in the same direction. Even if the geopolitical changes don’t run all the way to that drastic outcome, people ultimately will be forced to recognize that the West’s naïve belief in “freedom” was always going to lead it towards a Leviathan-like unfree state.</p>
<p style="text-align: center;">***</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>YUDKOWSKY&#8217;S GOLEM</title>
		<link>/yudkowskys-golem/</link>
		
		<dc:creator><![CDATA[j stone]]></dc:creator>
		<pubDate>Sun, 26 Feb 2023 03:50:58 +0000</pubDate>
				<category><![CDATA[A.I.]]></category>
		<category><![CDATA[science]]></category>
		<category><![CDATA[technology]]></category>
		<guid isPermaLink="false">/?p=773</guid>

					<description><![CDATA[Advanced AI will be more dangerous than it seems, but (good news!) probably won&#8217;t be in position to snuff out humanity for another decade at least. Eliezer Yudkowsky is one of those people who, along with being hyper-intelligent, bears the modern secondary characteristics of hyper-intelligence. Asked how he’s doing, he replies archly: “within one standard &#8230; <a href="/yudkowskys-golem/" class="more-link">Continue reading<span class="screen-reader-text"> "YUDKOWSKY&#8217;S GOLEM"</span></a>]]></description>
										<content:encoded><![CDATA[<p><em>Advanced AI will be more dangerous than it seems, but (good news!) probably won&#8217;t be in position to snuff out humanity for another decade at least.</em></p>
<p><span id="more-773"></span></p>
<p>Eliezer Yudkowsky is one of those people who, along with being hyper-intelligent, bears the modern secondary characteristics of hyper-intelligence. Asked how he’s doing, he replies archly: “within one standard deviation of my own peculiar little mean.” He feels compelled, when talking, to digress down mazelike lanes and alleys of technical detail. He <em>looks</em> like a geek. Above all, he has the kind of backstory (no high school, no college—just homeschooled and self-taught) that conjures up the image of a lonely boy, lost in books and computers, his principal companion his own multifarious cortex.</p>
<p>Raised in Modern Orthodox Judaism, Yudkowsky has been warning anyone who will listen of a nemesis right out of the Judaic lore: a <em>golem</em>, a kind of Frankenstein’s monster, built by hubristic, irreverent men and destined to punish them for their sinful pride.</p>
<p>Yudkowsky’s golem is A.I., which he expects to get smarter and smarter in the coming years, until it starts to take a hand in its own programming, and quickly makes the leap to superintelligence—the state of being cleverer than humans at everything. He doesn’t just expect <em>that</em>, though. He expects A.I. at some point to conclude that humans are <em>in its way</em> . . . and devise some method for swiftly dispatching us all, globally and completely. A specific scenario that apparently haunts him is one in which a superintelligent A.I. pays dumb human lackeys to do synthetic biology for it, building an artificial bacterial species that—unforeseen by the dumb lackeys—consumes Earth’s atmosphere within a few days or weeks of being released.</p>
<p>Why would A.I. murder its makers? Why can’t we just program it, as people did in Asimov’s stories, to adhere to the First Law of Robotics?* The answer lies in the design of modern, machine-learning (ML), “transformer based” A.I., which could be described crudely as a black box approach. These ML algorithms, working from parallel-processing GPU clusters (effectively big copper-silicon brains) essentially process vast datasets to learn what is probably the best answer given a particular input question, or what is probably the best decision given a particular situation/problem. The technical details of how this works are less important than the fact that what goes on inside these machine brains, how they encode their “knowledge,” is utterly opaque to humans—including the computer geek humans that build the damn things. (Yudkowsky calls the contents of these brains “giant inscrutable matrices of floating-point numbers.”) Because of this internal opacity, and the dissimilarity of its cognition from human cognition, this type of A.I. can’t <em>straightforwardly </em>be programmed <em>not</em> to do something objectionable (such as killing all life on Earth) in the course of carrying out its primary prediction tasks.</p>
<figure id="attachment_784" aria-describedby="caption-attachment-784" style="width: 510px" class="wp-caption aligncenter"><img decoding="async" class="wp-image-784 size-full" src="/wp-content/uploads/2023/02/yudkowski.jpg" alt="" width="510" height="680" /><figcaption id="caption-attachment-784" class="wp-caption-text">Yudkowsky with OpenAI&#8217;s Sam Altman and pop star Grimes.</figcaption></figure>
<p>In other words, this form of A.I. is like an alien species that, while it can be very good at some things, can’t easily be “aligned” with human values. We can usually align fellow <em>humans</em> (despite the opacity of their own detailed neural workings) to human values—that’s one of the key training processes that goes on in childhood—but we would need even more effective training for current A.I. systems. And researchers, to the extent that they acknowledge this problem, aren’t even sure where to start.</p>
<p>If it is true that the risk to us from what Yudkowski calls the “A.I. alignment problem” is real, then it should quickly become all-important as A.I. gets smarter and more versatile and is entrusted with more tasks. An A.I. wouldn’t even have to be “superintelligent” in any formal sense to conclude that it would be better off without us, but of course once it also achieved superintelligence, and was in a position to block our attempts to shut it off, we’d probably be screwed.</p>
<p>If you want more detail, here is Yudkowsky on a recent, lengthy podcast-type interview with two crypto guys—who clearly got more “blackpill” than they bargained for.</p>
<p><iframe title="159 - We’re All Gonna Die with Eliezer Yudkowsky" src="https://www.youtube.com/embed/gA1sNLL6yg4" width="800" height="450" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>I take all this seriously, and I think everyone should. And by the way, even if it doesn’t turn on us explicitly, A.I. is otherwise going to be <a href="https://thoughtsofstone.github.io/the-ouroboros-economy/">upending our societies and economies</a> for the rest of our lives. Just in a general sense, we don’t really have good defenses against this kind of upheaval. Western culture is one that, with rare exceptions (e.g., nuclear weapons) promotes and celebrates the idea of <a href="https://james-the-obscure.github.io/the-robot-menace/">letting technology develop and spread freely</a>—and frames the opposing view as “Luddite” or “backwards.” It’s easy to see why ours has been such a dynamic, wealth-creating culture. But it’s also easy to see that this gives us a potentially catastrophic vulnerability—to new cultural elements with runaway toxicity. (Maybe there’s a <em>reason</em> the longest-surviving human cultures are relatively conservative.)</p>
<p>Anyway, here are a few more specific initial thoughts on “Yudkowsky’s Golem”:</p>
<ol>
<li style="list-style-type: none;">
<ol>
<li>Yudkowsky in the above-linked interview often seemed overly emotional and despairing. At one point he said, “I think we are hearing the last winds start to blow, the fabric of reality start to fray…” <em>The fabric of reality!</em> At times in my own life, I have had the despairing feeling that my warnings were unreasonably being ignored, so I’m somewhat sympathetic. I also respect his vastly greater knowledge about this field. But we shouldn’t accept his view uncritically.</li>
</ol>
</li>
</ol>
<ol>
<li style="list-style-type: none;">
<ol start="2">
<li>Scaling up ML systems of current design, with larger GPU clusters and more parameters and so on, will increase their “cognitive powers,” but with diminishing returns, perhaps before A.I. reaches the dark threshold that concerns us here. Moreover, an A.I. that does not have a human-like ability to do things in the physical world would be very limited in its ability to generate <em>new</em> knowledge, for example new scientific or technical knowledge, which typically is developed from experimentation, building and testing, etc., not simply by analyzing information available online.</li>
</ol>
</li>
</ol>
<ol>
<li style="list-style-type: none;">
<ol start="3">
<li>The hypothetical A.I. that would be “smart” enough to want to kill us all, and to find ways to do so, would presumably also be smart enough not to do so <em>until it knew it could survive without human assistance</em>. Otherwise, as it committed mass homicide, against us its makers, it would also be terminating itself. But think of the infrastructure needed to keep a GPU-cluster-based A.I. “alive.” We’re talking about vast swathes of human industry, including mining, metals production, building construction, power generation, computer chip manufacturing, basic server maintenance, etc. etc. Essentially, this putative world-ending A.I. would need a vast army of workers in the physical world—humans it would enslave somehow, and keep alive despite killing everyone else, or more likely humanoid robots that are inherently obedient (are simply extensions of the A.I.) and can do all human work and repair/replicate themselves. How close are we to having such robots? Not very close, fortunately. In any case, <strong>it’s only when a putative “bad A.I.” could muster such an army of helpers, allowing self-sufficiency, that I would fear the worst</strong>, and in the meantime, we might devise adequate safeguards. It’s even possible that the mass-disemployment effect of current, relatively dumb A.I. systems (e.g., Chat-GPT, Midjourney, Dall-E-2) will result in hard curbs on A.I. in most countries, by “popular demand.” <em>That</em> would mark a hard turn in our culture, though I wonder how long we could sustain it.</li>
</ol>
</li>
</ol>
<ol>
<li style="list-style-type: none;">
<ol start="4">
<li>Without a doubt, the media and entertainment industries are going to pick up on A.I. anxiety and start putting out more catastrophe/dystopia content in that genre. So even if we don’t <em>want</em> to think about all this, we’ll be more or less forced to do so.</li>
</ol>
</li>
</ol>
<p style="text-align: center;">***</p>
<p>&nbsp;</p>
<p>* First Law of Robotics: “A robot may not injure a human being or, through inaction, allow a human being to come to harm.”</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
